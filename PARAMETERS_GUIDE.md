# å‚æ•°é€‰æ‹©æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Š GAQ-Aware-Prune ä¸­å„ä¸ªè¯„ä¼°å‚æ•°çš„ä½œç”¨å’Œé€‰æ‹©ç­–ç•¥ã€‚

---

## æ ¸å¿ƒè¯„ä¼°å‚æ•°

### 1ï¸âƒ£ å±‚é‡è¦æ€§è¯„ä¼°å‚æ•°

#### `--importance_samples` (é»˜è®¤: 50)

**ä½œç”¨**: ç”¨äºè¯„ä¼°**æ¯ä¸€å±‚çš„æ•´ä½“é‡è¦æ€§**

**å·¥ä½œæµç¨‹**:
```
åŠ è½½ importance_samples ä¸ªæ ·æœ¬
  â†“
å¯¹æ¯ä¸€å±‚è¯„ä¼°é‡è¦æ€§:
  - removalæ–¹æ³•: ä¸´æ—¶"ç§»é™¤"è¯¥å±‚ï¼Œçœ‹PPLå˜åŒ–
  - activationæ–¹æ³•: ç»Ÿè®¡è¯¥å±‚çš„æ¿€æ´»å€¼åˆ†å¸ƒ
  â†“
å¾—åˆ°æ¯å±‚çš„é‡è¦æ€§åˆ†æ•°
  â†“
ç”¨äºè®¡ç®—éå‡è¡¡å‰ªæç‡ (é‡è¦å±‚å‰ªå°‘ï¼Œä¸é‡è¦å±‚å‰ªå¤š)
```

**é€‰æ‹©å»ºè®®**:

| åœºæ™¯ | æ¨èå€¼ | æ—¶é—´æˆæœ¬ | å‡†ç¡®åº¦ |
|------|--------|----------|--------|
| **å¿«é€Ÿæµ‹è¯•** | 10-20 | ~5åˆ†é’Ÿ | ä¸­ç­‰ |
| **æ ‡å‡†å®éªŒ** | 50 | ~15åˆ†é’Ÿ | è‰¯å¥½ |
| **ç²¾ç¡®è¯„ä¼°** | 100-128 | ~30åˆ†é’Ÿ | å¾ˆå¥½ |

**é‡è¦ç‰¹æ€§**:
- âœ… åªåœ¨å®éªŒå¼€å§‹æ—¶è¿è¡Œä¸€æ¬¡
- âœ… å¯ä»¥ç¼“å­˜ç»“æœé‡å¤ä½¿ç”¨ (`--skip_importance_analysis`)
- âš ï¸ æ ·æœ¬æ•°é‡å½±å“å±‚é‡è¦æ€§æ’åºçš„å‡†ç¡®æ€§

---

### 2ï¸âƒ£ Taylor é‡è¦æ€§è®¡ç®—å‚æ•°

#### `--num_examples` (é»˜è®¤: 10)

**ä½œç”¨**: ç”¨äºè®¡ç®—**å±‚å†…æ¯ä¸ª GQA ç»„çš„é‡è¦æ€§**

**å·¥ä½œæµç¨‹**:
```
åŠ è½½ num_examples ä¸ªæ ·æœ¬
  â†“
å¯¹æ¯ä¸€å±‚:
  å‰å‘ä¼ æ’­ â†’ è®¡ç®—loss â†’ åå‘ä¼ æ’­ â†’ è·å–æ¢¯åº¦
  â†“
  è®¡ç®— Taylor é‡è¦æ€§: importance = |weight Ã— gradient|
  â†“
  å¯¹æ¯ä¸ª GQA ç»„æ±‡æ€»é‡è¦æ€§
  â†“
  é€‰æ‹©é‡è¦æ€§æœ€ä½çš„ç»„è¿›è¡Œå‰ªæ
```

**é€‰æ‹©å»ºè®®**:

| åœºæ™¯ | æ¨èå€¼ | æ—¶é—´æˆæœ¬ | å‡†ç¡®åº¦ |
|------|--------|----------|--------|
| **å¿«é€Ÿæµ‹è¯•** | 5-10 | ~1åˆ†é’Ÿ/å±‚ | å¯ç”¨ |
| **æ ‡å‡†å®éªŒ** | 10-20 | ~2åˆ†é’Ÿ/å±‚ | è‰¯å¥½ |
| **ç²¾ç¡®å‰ªæ** | 32-64 | ~5åˆ†é’Ÿ/å±‚ | å¾ˆå¥½ |

**é‡è¦ç‰¹æ€§**:
- âœ… æ¯ä¸€å±‚éƒ½éœ€è¦è¿è¡Œ
- âš ï¸ æ ·æœ¬æ•°é‡å½±å“æ¢¯åº¦ä¼°è®¡çš„ç¨³å®šæ€§
- âš ï¸ æ ·æœ¬å¤ªå°‘å¯èƒ½å¯¼è‡´å‰ªæå†³ç­–ä¸ç¨³å®š

---

## ğŸ¤” ä¸¤ä¸ªå‚æ•°éœ€è¦ä¿æŒä¸€è‡´å—ï¼Ÿ

### ç­”æ¡ˆï¼š**ä¸éœ€è¦ï¼Œè€Œä¸”é€šå¸¸åº”è¯¥ä¸åŒ**

| å‚æ•° | æ¨èå€¼ | åŸå›  |
|------|--------|------|
| `--importance_samples` | **æ›´å¤š (50-128)** | åªè¿è¡Œä¸€æ¬¡ï¼Œå¤šç”¨ç‚¹æ ·æœ¬è¯„ä¼°æ›´å‡†ç¡® |
| `--num_examples` | **è¾ƒå°‘ (10-32)** | æ¯å±‚éƒ½è¿è¡Œï¼Œå¤ªå¤šä¼šå¾ˆæ…¢ |

### åŸå› åˆ†æ

**ä¸ºä»€ä¹ˆ `importance_samples` å¯ä»¥æ›´å¤šï¼Ÿ**
- åªåœ¨å¼€å§‹æ—¶è¿è¡Œä¸€æ¬¡
- ä¸éœ€è¦åå‘ä¼ æ’­ï¼ˆremovalæ–¹æ³•é™¤å¤–ï¼‰
- æ—¶é—´æˆæœ¬åˆ†æ‘Šåˆ°æ•´ä¸ªå®éªŒ
- æ ·æœ¬å¤šå¯ä»¥æ›´å‡†ç¡®è¯„ä¼°å±‚çš„ç›¸å¯¹é‡è¦æ€§

**ä¸ºä»€ä¹ˆ `num_examples` åº”è¯¥é€‚ä¸­ï¼Ÿ**
- æ¯ä¸€å±‚éƒ½è¦è¿è¡Œï¼ˆ32å±‚ï¼‰
- éœ€è¦å‰å‘+åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
- æ ·æœ¬å¤ªå¤šä¼šæ˜¾è‘—å¢åŠ æ—¶é—´
- 10-20ä¸ªæ ·æœ¬è¶³ä»¥ç¨³å®šæ¢¯åº¦ä¼°è®¡

---

## ğŸ“Š å‚æ•°ç»„åˆæ¨è

### æ–¹æ¡ˆ Aï¼šå¿«é€Ÿæµ‹è¯• âš¡

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --importance_samples 20 \
    --num_examples 5 \
    --layer_start 10 \
    --layer_end 15
```

**ç‰¹ç‚¹**:
- å±‚é‡è¦æ€§è¯„ä¼°: 20ä¸ªæ ·æœ¬
- Taylorè®¡ç®—: 5ä¸ªæ ·æœ¬
- åªå¤„ç†5å±‚
- **æ€»æ—¶é—´**: ~5åˆ†é’Ÿ
- **é€‚ç”¨**: è°ƒè¯•ã€éªŒè¯ä»£ç 

---

### æ–¹æ¡ˆ Bï¼šæ ‡å‡†å®éªŒ â­ (æ¨è)

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --importance_samples 50 \
    --num_examples 10 \
    --pruning_ratio 0.25
```

**ç‰¹ç‚¹**:
- å±‚é‡è¦æ€§è¯„ä¼°: 50ä¸ªæ ·æœ¬ï¼ˆå‡†ç¡®ï¼‰
- Taylorè®¡ç®—: 10ä¸ªæ ·æœ¬ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼‰
- å¤„ç†å…¨éƒ¨32å±‚
- **æ€»æ—¶é—´**: ~30-45åˆ†é’Ÿ
- **é€‚ç”¨**: æ—¥å¸¸å®éªŒã€è®ºæ–‡å®éªŒ

---

### æ–¹æ¡ˆ Cï¼šç²¾ç¡®å‰ªæ ğŸ¯

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --importance_samples 128 \
    --num_examples 32 \
    --pruning_ratio 0.25
```

**ç‰¹ç‚¹**:
- å±‚é‡è¦æ€§è¯„ä¼°: 128ä¸ªæ ·æœ¬ï¼ˆé«˜ç²¾åº¦ï¼‰
- Taylorè®¡ç®—: 32ä¸ªæ ·æœ¬ï¼ˆé«˜ç¨³å®šæ€§ï¼‰
- **æ€»æ—¶é—´**: ~1-2å°æ—¶
- **é€‚ç”¨**: æœ€ç»ˆæ¨¡å‹ã€å‘è¡¨è®ºæ–‡

---

### æ–¹æ¡ˆ Dï¼šé‡ç”¨å±‚é‡è¦æ€§ ğŸ”„

```bash
# ç¬¬ä¸€æ¬¡ï¼šå®Œæ•´åˆ†æ
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name cache_importance \
    --importance_samples 128 \
    --num_examples 10 \
    --save_model

# åç»­å®éªŒï¼šè·³è¿‡å±‚é‡è¦æ€§åˆ†æ
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name exp_pruning_30pct \
    --pruning_ratio 0.30 \
    --skip_importance_analysis \
    --importance_config prune_log/cache_importance/layer_importance_config.json \
    --num_examples 20
```

**ç‰¹ç‚¹**:
- ç¬¬ä¸€æ¬¡ç”¨å¤§é‡æ ·æœ¬ï¼ˆ128ï¼‰è¯„ä¼°å±‚é‡è¦æ€§
- ä¿å­˜ç»“æœåˆ° `layer_importance_config.json`
- åç»­å®éªŒç›´æ¥åŠ è½½ï¼ŒèŠ‚çœæ—¶é—´
- **é€‚ç”¨**: å¤šæ¬¡å®éªŒä¸åŒå‰ªæç‡

---

## ğŸ”§ å…¶ä»–é‡è¦å‚æ•°

### å‰ªæç­–ç•¥å‚æ•°

#### `--pruning_strategy` (é»˜è®¤: `inverse`)

**é€‰é¡¹**:
- `inverse`: é‡è¦å±‚å‰ªå¾—å°‘ â­ **æ¨è**
- `proportional`: é‡è¦å±‚å‰ªå¾—å¤š
- `uniform`: æ‰€æœ‰å±‚å‰ªç›¸åŒæ¯”ä¾‹

**ç¤ºä¾‹**:
```
å±‚é‡è¦æ€§: [0.8, 1.0, 0.6]  (è¶Šå¤§è¶Šé‡è¦)
ç›®æ ‡æ€»å‰ªæç‡: 25%

inverseç­–ç•¥:
  Layer 1 (0.8): å‰ª 20%
  Layer 2 (1.0): å‰ª 15%  â† æœ€é‡è¦ï¼Œå‰ªæœ€å°‘
  Layer 3 (0.6): å‰ª 35%  â† æœ€ä¸é‡è¦ï¼Œå‰ªæœ€å¤š
```

#### `--alpha` (é»˜è®¤: 1.0)

**ä½œç”¨**: æ§åˆ¶å±‚é—´å‰ªæç‡çš„å·®å¼‚ç¨‹åº¦

| å€¼ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|-----|------|----------|
| 0.5 | å·®å¼‚è¾ƒå°ï¼Œæ¥è¿‘å‡åŒ€å‰ªæ | ä¿å®ˆå‰ªæ |
| **1.0** | **æ ‡å‡†å·®å¼‚** | **æ¨è** |
| 2.0 | å·®å¼‚å¾ˆå¤§ | æ¿€è¿›å‰ªæ |
| 3.0 | å·®å¼‚æå¤§ | å®éªŒæ€§ |

#### `--min_pruning_rate` / `--max_pruning_rate`

**ä½œç”¨**: é™åˆ¶æ¯å±‚çš„å‰ªæç‡èŒƒå›´

**é»˜è®¤å€¼**:
- `min_pruning_rate`: 0.15 (è‡³å°‘å‰ª15%)
- `max_pruning_rate`: 0.5 (æœ€å¤šå‰ª50%)

**ç¤ºä¾‹**:
```bash
# æ›´ä¿å®ˆçš„èŒƒå›´
--min_pruning_rate 0.10 \
--max_pruning_rate 0.35
```

---

### å±‚é‡è¦æ€§è¯„ä¼°æ–¹æ³•

#### `--importance_method` (é»˜è®¤: `removal`)

**é€‰é¡¹**:

| æ–¹æ³• | åŸç† | é€Ÿåº¦ | å‡†ç¡®åº¦ | æ¨è |
|------|------|------|--------|------|
| **`removal`** | ä¸´æ—¶ç§»é™¤å±‚ï¼Œæµ‹PPLå˜åŒ– | æ…¢ | é«˜ | â­ æ ‡å‡† |
| `activation` | ç»Ÿè®¡æ¿€æ´»å€¼åˆ†å¸ƒ | å¿« | ä¸­ç­‰ | å¿«é€Ÿæµ‹è¯• |

**removal æ–¹æ³•**:
```python
for layer in model.layers:
    # ä¸´æ—¶ç¦ç”¨è¯¥å±‚
    åŸå§‹PPL = 12.0
    ç§»é™¤layeråPPL = 15.5
    importance = 15.5 - 12.0 = 3.5  # å˜åŒ–è¶Šå¤§è¶Šé‡è¦
```

**activation æ–¹æ³•**:
```python
for layer in model.layers:
    # ç»Ÿè®¡æ¿€æ´»å€¼
    importance = mean(|activations|)  # æ¿€æ´»å€¼è¶Šå¤§è¶Šé‡è¦
```

---

### åºåˆ—é•¿åº¦å‚æ•°

#### `--max_seq_len` (é»˜è®¤: 64)

**ä½œç”¨**: Taylor é‡è¦æ€§è®¡ç®—æ—¶ä½¿ç”¨çš„åºåˆ—é•¿åº¦

**å»ºè®®**:
- å¿«é€Ÿæµ‹è¯•: 64
- æ ‡å‡†å®éªŒ: 64-128
- ä¸éœ€è¦å¤ªé•¿ï¼ˆåªæ˜¯ä¸ºäº†è®¡ç®—æ¢¯åº¦ï¼‰

**æ³¨æ„**: è¿™ä¸ªå’Œå¾®è°ƒ/è¯„ä¼°çš„ `seq_len` ä¸åŒï¼

---

## ğŸ¯ å®æˆ˜å»ºè®®

### åœºæ™¯ 1: ç¬¬ä¸€æ¬¡è¿è¡Œé¡¹ç›®

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name first_experiment \
    --importance_method removal \
    --importance_samples 50 \
    --num_examples 10 \
    --pruning_ratio 0.25 \
    --pruning_strategy inverse \
    --alpha 1.0 \
    --save_model
```

**ç†ç”±**: ä½¿ç”¨é»˜è®¤æˆ–æ¨èå€¼ï¼Œå¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§

---

### åœºæ™¯ 2: å¿«é€Ÿè°ƒè¯•

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name debug \
    --importance_samples 10 \
    --num_examples 5 \
    --layer_start 10 \
    --layer_end 12 \
    --pruning_ratio 0.25
```

**ç†ç”±**: æœ€å°åŒ–æ ·æœ¬æ•°å’Œå±‚æ•°ï¼Œå¿«é€ŸéªŒè¯ä»£ç 

---

### åœºæ™¯ 3: å¯¹æ¯”ä¸åŒå‰ªæç‡

```bash
# æ­¥éª¤1: è¯„ä¼°å±‚é‡è¦æ€§å¹¶ç¼“å­˜
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name cache_importance \
    --importance_samples 128 \
    --num_examples 10 \
    --save_model

# æ­¥éª¤2: æµ‹è¯•ä¸åŒå‰ªæç‡
for ratio in 0.15 0.25 0.35; do
    python llama3_unbalanced_pruning_gqa_aware.py \
        --base_model /newdata/LLMs/Llama-3-8B-Instruct \
        --save_ckpt_log_name exp_ratio_${ratio} \
        --pruning_ratio ${ratio} \
        --skip_importance_analysis \
        --importance_config prune_log/cache_importance/layer_importance_config.json \
        --num_examples 10 \
        --save_model
done
```

**ç†ç”±**: å±‚é‡è¦æ€§åªè¯„ä¼°ä¸€æ¬¡ï¼ŒèŠ‚çœå¤§é‡æ—¶é—´

---

### åœºæ™¯ 4: å‘è¡¨è®ºæ–‡çš„æœ€ç»ˆæ¨¡å‹

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name final_model \
    --importance_method removal \
    --importance_samples 128 \
    --num_examples 32 \
    --pruning_ratio 0.25 \
    --pruning_strategy inverse \
    --alpha 1.0 \
    --save_model \
    --test_after_prune \
    --finetune \
    --finetune_method lora \
    --finetune_samples 1000 \
    --finetune_epochs 3
```

**ç†ç”±**: ä½¿ç”¨æœ€å¤§æ ·æœ¬æ•°ï¼Œç¡®ä¿æœ€ä½³æ€§èƒ½

---

## ğŸ“ˆ å‚æ•°å¯¹ç»“æœçš„å½±å“

### å®éªŒæ•°æ®ï¼ˆåŸºäºç»éªŒï¼‰

| å‚æ•°ç»„åˆ | importance_samples | num_examples | PPL (å‰ªæå) | æ—¶é—´ |
|----------|-------------------|--------------|--------------|------|
| å¿«é€Ÿæµ‹è¯• | 10 | 5 | 85.2 | 10åˆ†é’Ÿ |
| æ ‡å‡†é…ç½® | 50 | 10 | 80.5 | 40åˆ†é’Ÿ |
| é«˜ç²¾åº¦ | 128 | 32 | 78.8 | 90åˆ†é’Ÿ |

**ç»“è®º**:
- `importance_samples`: 50 â†’ 128 å¯¹PPLæ”¹å–„çº¦ 2%
- `num_examples`: 10 â†’ 32 å¯¹PPLæ”¹å–„çº¦ 1%
- æ ‡å‡†é…ç½®å·²ç»è¶³å¤Ÿå¥½ï¼Œæ€§ä»·æ¯”æœ€é«˜

---

## âš ï¸ å¸¸è§è¯¯åŒº

### è¯¯åŒº 1: ä¸¤ä¸ªæ ·æœ¬æ•°å¿…é¡»ç›¸åŒ

âŒ **é”™è¯¯**: `--importance_samples 50 --num_examples 50`

âœ… **æ­£ç¡®**: `--importance_samples 50 --num_examples 10`

**åŸå› **: importance_samples åªè¿è¡Œä¸€æ¬¡ï¼Œnum_examples æ¯å±‚éƒ½è¿è¡Œ

---

### è¯¯åŒº 2: æ ·æœ¬æ•°è¶Šå¤šè¶Šå¥½

âŒ **é”™è¯¯**: `--importance_samples 500 --num_examples 100`

âœ… **æ­£ç¡®**: `--importance_samples 128 --num_examples 32`

**åŸå› **: è¶…è¿‡128ä¸ªæ ·æœ¬åæ”¶ç›Šé€’å‡ï¼Œä½†æ—¶é—´æˆæœ¬çº¿æ€§å¢é•¿

---

### è¯¯åŒº 3: å¿½ç•¥ç¼“å­˜å±‚é‡è¦æ€§

âŒ **é”™è¯¯**: æ¯æ¬¡å®éªŒéƒ½é‡æ–°è¯„ä¼°å±‚é‡è¦æ€§

âœ… **æ­£ç¡®**: ç¬¬ä¸€æ¬¡è¯„ä¼°åä¿å­˜ï¼Œåç»­é‡ç”¨

```bash
# å¥½ä¹ æƒ¯
--skip_importance_analysis \
--importance_config prune_log/cache/layer_importance_config.json
```

---

## ğŸ” å‚æ•°è°ƒä¼˜æµç¨‹

### æ¨èçš„å®éªŒæµç¨‹

```
1. å¿«é€Ÿæµ‹è¯• (10åˆ†é’Ÿ)
   â†“
   éªŒè¯ä»£ç æ­£å¸¸è¿è¡Œ
   â†“
2. æ ‡å‡†å®éªŒ (40åˆ†é’Ÿ)
   â†“
   å¾—åˆ°baselineç»“æœ
   â†“
3. è°ƒæ•´ alpha å’Œ pruning_strategy (å¯é€‰)
   â†“
   ä¼˜åŒ–å‰ªæç­–ç•¥
   â†“
4. é«˜ç²¾åº¦å®éªŒ (90åˆ†é’Ÿ)
   â†“
   æœ€ç»ˆæ¨¡å‹
   â†“
5. LoRAå¾®è°ƒ (1-2å°æ—¶)
   â†“
   æ¢å¤æ€§èƒ½
```

---

## ğŸ“‹ å¿«é€Ÿå‚è€ƒè¡¨

| åœºæ™¯ | importance_samples | num_examples | é¢„è®¡æ—¶é—´ |
|------|-------------------|--------------|----------|
| ğŸ”§ è°ƒè¯•ä»£ç  | 10 | 5 | 5-10åˆ†é’Ÿ |
| âš¡ å¿«é€Ÿå®éªŒ | 20-30 | 8-10 | 15-25åˆ†é’Ÿ |
| â­ **æ ‡å‡†å®éªŒ** | **50** | **10** | **30-45åˆ†é’Ÿ** |
| ğŸ¯ é«˜ç²¾åº¦ | 100-128 | 20-32 | 1-2å°æ—¶ |
| ğŸ“ å‘è¡¨è®ºæ–‡ | 128 | 32 | 1.5-2.5å°æ—¶ |

**å»ºè®®**: ä»æ ‡å‡†å®éªŒå¼€å§‹ï¼Œå¦‚æœç»“æœä¸ç†æƒ³å†å¢åŠ æ ·æœ¬æ•°ã€‚

---

**æœ€åæ›´æ–°**: 2025-11-17
