#!/usr/bin/env python3
"""
åŸºäºå…¨å±€æ€§ä»·æ¯”æ’åºçš„æ··åˆç»“æ„åŒ–å‰ªæ

æ ¸å¿ƒæ€æƒ³ï¼š
- å°†å‰ªæé—®é¢˜å»ºæ¨¡ä¸ºåˆ†æ•°èƒŒåŒ…é—®é¢˜
- Score = Importance / Cost
- å…¨å±€æ’åºï¼Œä¼˜å…ˆå‰ªé™¤"æ€§ä»·æ¯”"æœ€ä½çš„ groups
- è‡ªåŠ¨å®ç°æ·±åº¦å‰ªæï¼ˆå±‚ç§»é™¤ï¼‰+ å®½åº¦å‰ªæï¼ˆç¥ç»å…ƒå‰ªé™¤ï¼‰çš„æ··åˆç­–ç•¥
"""

import os
import torch
import argparse
import time
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

from core.methods.global_pruning import (
    build_global_group_table,
    select_groups_to_prune
)
from core.methods.gqa_aware import prune_attention_by_gqa_groups
from core.datasets.example_samples import get_examples
from evaluation.metrics.ppl import PPLMetric
from core.trainer.finetuner import FineTuner
from core.utils.logger import LoggerWithDepth


def apply_global_pruning(model, groups_to_prune_df, head_dim=128, gqa_ratio=4, logger=None):
    """
    æ ¹æ®å…¨å±€åˆ†æè¡¨æ‰§è¡Œå®é™…å‰ªæ

    Args:
        model: æ¨¡å‹
        groups_to_prune_df: è¦å‰ªæçš„ groups DataFrame
        head_dim: attention head ç»´åº¦
        gqa_ratio: Q:KV æ¯”ä¾‹
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        pruned_layers: è¢«å®Œå…¨å‰ªç©ºçš„å±‚åˆ—è¡¨
        pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    log("\n" + "="*60)
    log("æ‰§è¡Œå…¨å±€å‰ªæ")
    log("="*60)

    num_layers = len(model.model.layers)
    pruning_stats = {
        'attention': {},  # {layer_idx: (old_kv, new_kv)}
        'mlp': {},        # {layer_idx: (old_channels, new_channels)}
        'empty_layers': []
    }

    # æŒ‰å±‚ç»„ç»‡è¦å‰ªæçš„ groups
    layer_prune_info = {}
    for layer_idx in range(num_layers):
        layer_data = groups_to_prune_df[groups_to_prune_df['layer_idx'] == layer_idx]

        attn_groups = layer_data[layer_data['group_type'] == 'attention']['group_idx'].tolist()
        mlp_groups = layer_data[layer_data['group_type'] == 'mlp']['group_idx'].tolist()

        layer_prune_info[layer_idx] = {
            'attention': attn_groups,
            'mlp': mlp_groups
        }

    # æ‰§è¡Œå‰ªæ
    for layer_idx in range(num_layers):
        layer = model.model.layers[layer_idx]
        prune_info = layer_prune_info[layer_idx]

        log(f"\nå¤„ç† Layer {layer_idx}:")

        # ========== Attention å‰ªæ ==========
        attn_prune_indices = prune_info['attention']

        if len(attn_prune_indices) > 0:
            # è·å–å½“å‰ KV heads æ•°é‡
            num_kv_heads = layer.self_attn.num_key_value_heads

            # è®¡ç®—ä¿ç•™çš„ indices
            all_kv_indices = set(range(num_kv_heads))
            keep_kv_indices = sorted(list(all_kv_indices - set(attn_prune_indices)))

            old_q = layer.self_attn.num_heads
            old_kv = layer.self_attn.num_key_value_heads

            if len(keep_kv_indices) > 0:
                # æ‰§è¡Œå‰ªæ
                new_q, new_kv = prune_attention_by_gqa_groups(
                    layer,
                    keep_kv_indices,
                    head_dim=head_dim,
                    gqa_ratio=gqa_ratio
                )
                log(f"  Attention: {old_q}Q:{old_kv}KV â†’ {new_q}Q:{new_kv}KV")
                pruning_stats['attention'][layer_idx] = (old_kv, new_kv)
            else:
                # è¯¥å±‚ Attention è¢«å®Œå…¨å‰ªç©º
                log(f"  âš ï¸ Attention è¢«å®Œå…¨å‰ªç©ºï¼ˆ{old_kv} â†’ 0 KV headsï¼‰")
                pruning_stats['attention'][layer_idx] = (old_kv, 0)

        # ========== MLP å‰ªæ ==========
        mlp_prune_indices = prune_info['mlp']

        if len(mlp_prune_indices) > 0:
            intermediate_size = layer.mlp.gate_proj.out_features

            # è®¡ç®—ä¿ç•™çš„ indices
            all_mlp_indices = set(range(intermediate_size))
            keep_mlp_indices = sorted(list(all_mlp_indices - set(mlp_prune_indices)))

            if len(keep_mlp_indices) > 0:
                # æ‰§è¡Œ MLP å‰ªæ
                keep_mlp_indices_tensor = torch.tensor(keep_mlp_indices, device=layer.mlp.gate_proj.weight.device)

                # å‰ªæ gate_proj å’Œ up_projï¼ˆä¿ç•™å¯¹åº”çš„è¡Œï¼‰
                layer.mlp.gate_proj.weight = torch.nn.Parameter(
                    layer.mlp.gate_proj.weight[keep_mlp_indices_tensor, :]
                )
                layer.mlp.up_proj.weight = torch.nn.Parameter(
                    layer.mlp.up_proj.weight[keep_mlp_indices_tensor, :]
                )

                # å‰ªæ down_projï¼ˆä¿ç•™å¯¹åº”çš„åˆ—ï¼‰
                layer.mlp.down_proj.weight = torch.nn.Parameter(
                    layer.mlp.down_proj.weight[:, keep_mlp_indices_tensor]
                )

                # æ›´æ–° intermediate_size
                new_intermediate_size = len(keep_mlp_indices)
                layer.mlp.gate_proj.out_features = new_intermediate_size
                layer.mlp.up_proj.out_features = new_intermediate_size
                layer.mlp.down_proj.in_features = new_intermediate_size

                log(f"  MLP: {intermediate_size} â†’ {new_intermediate_size} channels")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, new_intermediate_size)
            else:
                # è¯¥å±‚ MLP è¢«å®Œå…¨å‰ªç©º
                log(f"  âš ï¸ MLP è¢«å®Œå…¨å‰ªç©ºï¼ˆ{intermediate_size} â†’ 0 channelsï¼‰")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, 0)

        # æ£€æŸ¥æ˜¯å¦æ•´å±‚è¢«å‰ªç©º
        attn_empty = (layer_idx in pruning_stats['attention'] and
                     pruning_stats['attention'][layer_idx][1] == 0)
        mlp_empty = (layer_idx in pruning_stats['mlp'] and
                    pruning_stats['mlp'][layer_idx][1] == 0)

        if attn_empty and mlp_empty:
            log(f"  ğŸ”´ Layer {layer_idx} è¢«å®Œå…¨å‰ªç©ºï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰")
            pruning_stats['empty_layers'].append(layer_idx)

    return pruning_stats


def remove_empty_layers(model, empty_layers, logger=None):
    """
    ç§»é™¤è¢«å®Œå…¨å‰ªç©ºçš„å±‚

    Args:
        model: æ¨¡å‹
        empty_layers: è¦ç§»é™¤çš„å±‚ç´¢å¼•åˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    if len(empty_layers) == 0:
        log("\nâœ“ æ²¡æœ‰å±‚è¢«å®Œå…¨å‰ªç©ºï¼Œè·³è¿‡å±‚ç§»é™¤")
        return

    log(f"\n{'='*60}")
    log(f"ç§»é™¤å®Œå…¨å‰ªç©ºçš„å±‚")
    log(f"{'='*60}")
    log(f"è¦ç§»é™¤çš„å±‚: {empty_layers}")

    # åˆ›å»ºä¿ç•™çš„å±‚åˆ—è¡¨
    num_layers = len(model.model.layers)
    keep_layers = [i for i in range(num_layers) if i not in empty_layers]

    # é‡å»º layers åˆ—è¡¨
    new_layers = torch.nn.ModuleList([model.model.layers[i] for i in keep_layers])
    model.model.layers = new_layers

    # æ›´æ–°é…ç½®
    model.config.num_hidden_layers = len(keep_layers)

    log(f"âœ“ å±‚æ•°: {num_layers} â†’ {len(keep_layers)}")


def main():
    parser = argparse.ArgumentParser(description='åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_model', type=str, required=True,
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_ckpt_log_name', type=str, default='llama_global_prune',
                       help='å®éªŒåç§°')

    # å‰ªæå‚æ•°
    parser.add_argument('--pruning_ratio', type=float, default=0.25,
                       help='ç›®æ ‡å‰ªæç‡ï¼ˆç›¸å¯¹äºæ¨¡å‹æ€»å‚æ•°ï¼‰')
    parser.add_argument('--importance_method', type=str, default='taylor',
                       choices=['taylor', 'wanda'],
                       help='é‡è¦æ€§è®¡ç®—æ–¹æ³•')
    parser.add_argument('--num_samples', type=int, default=128,
                       help='ç”¨äºè®¡ç®—é‡è¦æ€§çš„æ ·æœ¬æ•°')
    parser.add_argument('--gradient_batch_size', type=int, default=4,
                       help='æ¢¯åº¦è®¡ç®—æ—¶çš„æ‰¹æ¬¡å¤§å°ï¼ˆç”¨äºèŠ‚çœå†…å­˜ï¼‰')
    parser.add_argument('--remove_empty_layers', action='store_true',
                       help='æ˜¯å¦ç§»é™¤è¢«å®Œå…¨å‰ªç©ºçš„å±‚ï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰')

    # GQA é…ç½®
    parser.add_argument('--head_dim', type=int, default=128,
                       help='Attention head ç»´åº¦')
    parser.add_argument('--gqa_ratio', type=int, default=4,
                       help='Q:KV æ¯”ä¾‹')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--test_before_prune', action='store_true',
                       help='å‰ªæå‰è¯„ä¼°åŸºçº¿ PPL')
    parser.add_argument('--test_after_prune', action='store_true',
                       help='å‰ªæåè¯„ä¼° PPL')

    # å¾®è°ƒå‚æ•°
    parser.add_argument('--finetune', action='store_true',
                       help='å‰ªæåè¿›è¡Œå¾®è°ƒ')
    parser.add_argument('--finetune_method', type=str, default='lora',
                       choices=['full', 'lora'],
                       help='å¾®è°ƒæ–¹æ³•')
    parser.add_argument('--finetune_samples', type=int, default=500,
                       help='å¾®è°ƒæ ·æœ¬æ•°')
    parser.add_argument('--finetune_lr', type=float, default=1e-4,
                       help='å¾®è°ƒå­¦ä¹ ç‡')
    parser.add_argument('--finetune_epochs', type=int, default=1,
                       help='å¾®è°ƒè½®æ•°')
    parser.add_argument('--lora_r', type=int, default=8,
                       help='LoRA rank')
    parser.add_argument('--lora_alpha', type=int, default=16,
                       help='LoRA alpha')

    # ä¿å­˜å‚æ•°
    parser.add_argument('--save_model', action='store_true',
                       help='ä¿å­˜å‰ªæåçš„æ¨¡å‹')

    # å…¶ä»–
    from core.utils.get_best_gpu import get_best_gpu
    # bestDevice= "cuda:"+str(get_best_gpu())
    bestDevice = "cpu"
    parser.add_argument('--device', type=str, default=bestDevice,
                       help='è®¾å¤‡')
    parser.add_argument('--layer_start', type=int, default=0,
                       help='èµ·å§‹å±‚ï¼ˆdebugç”¨ï¼‰')
    parser.add_argument('--layer_end', type=int, default=None,
                       help='ç»“æŸå±‚ï¼ˆdebugç”¨ï¼‰')

    args = parser.parse_args()

    # è®¾ç½® logger
    logger = LoggerWithDepth(
        env_name=args.save_ckpt_log_name,
        config=args.__dict__,
        root_dir='prune_log'
    )

    logger.log("="*60)
    logger.log("åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ")
    logger.log("="*60)
    logger.log(f"æ¨¡å‹: {args.base_model}")
    logger.log(f"å‰ªæç‡: {args.pruning_ratio:.1%}")
    logger.log(f"é‡è¦æ€§æ–¹æ³•: {args.importance_method}")

    # ========== Step 1: åŠ è½½æ¨¡å‹ ==========
    logger.log("\n[Step 1] åŠ è½½æ¨¡å‹...")

    # ä½¿ç”¨ device_map='auto' è®© transformers è‡ªåŠ¨ç®¡ç†å†…å­˜
    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map = args.device,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)

    # è·å–å®é™…ä½¿ç”¨çš„è®¾å¤‡
    if hasattr(model, 'hf_device_map'):
        logger.log(f"  æ¨¡å‹åˆ†å¸ƒ: {model.hf_device_map}")
        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å—çš„è®¾å¤‡ï¼ˆè¾“å…¥æ•°æ®åº”è¯¥å‘é€åˆ°è¿™é‡Œï¼‰
        first_device = next(iter(model.hf_device_map.values()))
        args.device = f'cuda:{first_device}' if isinstance(first_device, int) else first_device
        logger.log(f"  è¾“å…¥è®¾å¤‡: {args.device}")
    else:
        args.device = next(model.parameters()).device

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    logger.log(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    logger.log(f"  æ€»å‚æ•°é‡: {total_params:,}")

    # ========== Step 2: è¯„ä¼°åŸºçº¿ ==========
    if args.test_before_prune:
        logger.log("\n[Step 2] è¯„ä¼°åŸºçº¿ PPL...")
        baseline_ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device=args.device)
        logger.log(f"âœ“ åŸºçº¿ PPL: {baseline_ppl}")

    # ========== Step 3: è®¡ç®—æ¢¯åº¦ ==========
    if args.importance_method == 'taylor':
        logger.log(f"\n[Step 3] è®¡ç®—æ¢¯åº¦ï¼ˆTaylor importanceï¼‰...")
        logger.log(f"  åŠ è½½ {args.num_samples} ä¸ªæ ·æœ¬...")

        # åˆ†æ‰¹è®¡ç®—æ¢¯åº¦ä»¥èŠ‚çœå†…å­˜
        batch_size = args.gradient_batch_size
        num_batches = (args.num_samples + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        model.zero_grad()
        total_loss = 0.0
        start_time = time.time()

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(range(num_batches), desc="è®¡ç®—æ¢¯åº¦", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, args.num_samples)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # åŠ è½½å½“å‰æ‰¹æ¬¡
            input_ids = get_examples('wikitext', tokenizer, num_samples=current_batch_size, seq_len=128)
            input_ids = input_ids.to(args.device)

            # å‰å‘+åå‘ä¼ æ’­
            outputs = model(input_ids, labels=input_ids)
            loss = outputs.loss / num_batches  # å½’ä¸€åŒ–
            loss.backward()

            batch_time = time.time() - batch_start_time
            total_loss += loss.item() * num_batches

            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar.set_postfix({
                'loss': f'{loss.item() * num_batches:.4f}',
                'batch_time': f'{batch_time:.2f}s'
            })

            # æ¸…ç†å†…å­˜
            del input_ids, outputs, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¢¯åº¦è®¡ç®—å®Œæˆ")
        logger.log(f"  å¹³å‡ loss: {total_loss:.4f}")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

        activations = None
    else:
        logger.log("\n[Step 3] Wanda æ–¹æ³•æš‚ä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨ --importance_method taylor")
        return

    # ========== Step 4: æ„å»ºå…¨å±€åˆ†æè¡¨ ==========
    logger.log("\n[Step 4] æ„å»ºå…¨å±€ Group åˆ†æè¡¨...")

    layer_end = args.layer_end if args.layer_end else len(model.model.layers)

    df = build_global_group_table(
        model=model,
        importance_method=args.importance_method,
        activations=activations,
        layer_start=args.layer_start,
        layer_end=layer_end,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        device=args.device
    )

    logger.log(f"âœ“ åˆ†æè¡¨æ„å»ºå®Œæˆ")

    # ========== Step 5: é€‰æ‹©è¦å‰ªæçš„ groups ==========
    logger.log(f"\n[Step 5] æ ¹æ®å‰ªæç‡é€‰æ‹©è¦å‰ªæçš„ groups...")

    groups_to_prune = select_groups_to_prune(
        df=df,
        pruning_ratio=args.pruning_ratio,
        total_params=total_params
    )

    logger.log(f"âœ“ é€‰ä¸­ {len(groups_to_prune)} ä¸ª groups è¿›è¡Œå‰ªæ")

    # ä¿å­˜åˆ†æè¡¨
    table_path = os.path.join(logger.env_dir, 'global_group_table.csv')
    df.to_csv(table_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜: {table_path}")

    prune_table_path = os.path.join(logger.env_dir, 'groups_to_prune.csv')
    groups_to_prune.to_csv(prune_table_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜: {prune_table_path}")

    # ========== Step 6: æ‰§è¡Œå…¨å±€å‰ªæ ==========
    logger.log(f"\n[Step 6] æ‰§è¡Œå…¨å±€å‰ªæ...")

    pruning_stats = apply_global_pruning(
        model=model,
        groups_to_prune_df=groups_to_prune,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        logger=logger
    )

    logger.log("\nâœ“ å…¨å±€å‰ªæå®Œæˆ")

    # ========== Step 7: ç§»é™¤ç©ºå±‚ï¼ˆå¯é€‰ï¼‰==========
    if args.remove_empty_layers and len(pruning_stats['empty_layers']) > 0:
        logger.log(f"\n[Step 7] ç§»é™¤ç©ºå±‚...")
        remove_empty_layers(model, pruning_stats['empty_layers'], logger)

    # ========== Step 8: ç»Ÿè®¡å‰ªæç»“æœ ==========
    logger.log(f"\n{'='*60}")
    logger.log(f"å‰ªæç»Ÿè®¡")
    logger.log(f"{'='*60}")

    after_params = sum(p.numel() for p in model.parameters())
    actual_ratio = (total_params - after_params) / total_params

    logger.log(f"å‚æ•°ç»Ÿè®¡:")
    logger.log(f"  å‰ªæå‰: {total_params:,}")
    logger.log(f"  å‰ªæå: {after_params:,}")
    logger.log(f"  å®é™…å‰ªæç‡: {actual_ratio:.2%}")

    if len(pruning_stats['empty_layers']) > 0:
        logger.log(f"\nè‡ªåŠ¨æ·±åº¦å‰ªæ:")
        logger.log(f"  ç§»é™¤çš„å±‚: {pruning_stats['empty_layers']}")
        logger.log(f"  å‰©ä½™å±‚æ•°: {len(model.model.layers)}")

    # ========== Step 9: è¯„ä¼°å‰ªæå PPL ==========
    if args.test_after_prune:
        logger.log(f"\n[Step 9] è¯„ä¼°å‰ªæå PPL...")
        pruned_ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device=args.device)
        logger.log(f"âœ“ å‰ªæå PPL: {pruned_ppl}")

        if args.test_before_prune:
            degradation = (pruned_ppl['wikitext2 (wikitext-2-raw-v1)'] /
                          baseline_ppl['wikitext2 (wikitext-2-raw-v1)'] - 1) * 100
            logger.log(f"  PPL é€€åŒ–: {degradation:.2f}%")

    # ========== Step 10: å¾®è°ƒæ¢å¤ï¼ˆå¯é€‰ï¼‰==========
    if args.finetune:
        logger.log(f"\n[Step 10] å¾®è°ƒæ¢å¤...")

        finetuner = FineTuner(model, tokenizer, device=args.device, logger=logger)

        finetuner.finetune(
            dataset_name='wikitext',
            num_samples=args.finetune_samples,
            lr=args.finetune_lr,
            epochs=args.finetune_epochs,
            method=args.finetune_method,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha
        )

        logger.log(f"âœ“ å¾®è°ƒå®Œæˆ")

        # è¯„ä¼°å¾®è°ƒå PPL
        if args.test_after_prune:
            logger.log(f"\nè¯„ä¼°å¾®è°ƒå PPL...")
            finetuned_ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device=args.device)
            logger.log(f"âœ“ å¾®è°ƒå PPL: {finetuned_ppl}")

            if args.test_before_prune:
                final_degradation = (finetuned_ppl['wikitext2 (wikitext-2-raw-v1)'] /
                                    baseline_ppl['wikitext2 (wikitext-2-raw-v1)'] - 1) * 100
                logger.log(f"  æœ€ç»ˆ PPL é€€åŒ–: {final_degradation:.2f}%")

    # ========== Step 11: ä¿å­˜æ¨¡å‹ ==========
    if args.save_model:
        logger.log(f"\n[Step 11] ä¿å­˜æ¨¡å‹...")

        save_path = os.path.join(logger.env_dir, 'pytorch_model.bin')

        save_dict = {
            'model': model,
            'tokenizer': tokenizer,
            'pruning_stats': pruning_stats,
            'pruning_ratio': args.pruning_ratio,
            'actual_ratio': actual_ratio,
            'method': 'global_pruning',
            'config': args.__dict__
        }

        torch.save(save_dict, save_path)
        logger.log(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {save_path}")

    logger.log(f"\n{'='*60}")
    logger.log(f"âœ“ å…¨éƒ¨å®Œæˆï¼")
    logger.log(f"{'='*60}")


if __name__ == '__main__':
    main()
