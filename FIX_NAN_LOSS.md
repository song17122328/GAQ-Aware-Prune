# ä¿®å¤NaN Lossé—®é¢˜æŒ‡å—

## ğŸš¨ é—®é¢˜æè¿°

æ‚¨é‡åˆ°çš„é—®é¢˜ï¼š
- âœ… å‰ªæåPPL = 80.85ï¼ˆ**å¼‚å¸¸é«˜**ï¼Œæ­£å¸¸åº”ä¸º10-15ï¼‰
- âŒ å¾®è°ƒæ—¶Lossç«‹å³å˜æˆNaN
- âŒ å¾®è°ƒåPPLä¹Ÿæ˜¯NaNï¼Œæ¨¡å‹å®Œå…¨æŸå

## ğŸ” æ ¹æœ¬åŸå› 

**å‰ªæåPPL=80.85å·²ç»è¯´æ˜æ¨¡å‹æœ‰ä¸¥é‡é—®é¢˜**ã€‚æ­£å¸¸çš„å‰ªæåº”è¯¥ï¼š
- 25%å‰ªæç‡ï¼šPPL 11-13
- 30%å‰ªæç‡ï¼šPPL 13-15
- **>50 çš„PPL**: æ¨¡å‹å·²ç»ä¸¥é‡æŸå

å¯èƒ½çš„åŸå› ï¼š
1. âŒ å‰ªæç‡è¿‡é«˜ï¼ˆè¶…è¿‡æ¨¡å‹æ‰¿å—èƒ½åŠ›ï¼‰
2. âŒ å‰ªæè¿‡ç¨‹å‡ºç°é”™è¯¯
3. âŒ æŸäº›å±‚è¢«è¿‡åº¦å‰ªæ
4. âŒ GQAæ¯”ä¾‹æœªæ­£ç¡®ç»´æŠ¤

---

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### æ­¥éª¤1ï¼šè¯Šæ–­å½“å‰æ¨¡å‹

é¦–å…ˆè¿è¡Œè¯Šæ–­è„šæœ¬æ£€æŸ¥æ¨¡å‹æ˜¯å¦çœŸçš„æŸåï¼š

```bash
python diagnose_model.py \
    --model_path prune_log/llama3_pruned_finetuned/pytorch_model.bin \
    --test_forward
```

**é¢„æœŸè¾“å‡º**ï¼š
```
============================================================
è¯Šæ–­ç»“æœ
============================================================

æ€»å‚æ•°é‡: 6,363,025,408
âœ… æ— NaNå€¼
âœ… æ— Infå€¼
âœ… é›¶å€¼æ¯”ä¾‹æ­£å¸¸: 15.2%
âœ… æ— å¼‚å¸¸å¤§çš„å€¼

âœ… æ‰€æœ‰å±‚çœ‹èµ·æ¥æ­£å¸¸

============================================================
3. æµ‹è¯•å‰å‘ä¼ æ’­
============================================================
âœ… å‰å‘ä¼ æ’­æ­£å¸¸
   è¾“å‡ºshape: torch.Size([1, 7, 128256])
   è¾“å‡ºèŒƒå›´: [-15.23, 18.45]
```

**å¦‚æœå‘ç°NaN/Inf**ï¼šæ¨¡å‹åœ¨å‰ªææ—¶å°±å·²ç»æŸåï¼Œéœ€è¦é‡æ–°å‰ªæã€‚

---

### æ­¥éª¤2ï¼šé‡æ–°å‰ªæï¼ˆé™ä½å‰ªæç‡ï¼‰

**åŸå› **ï¼šæ‚¨çš„PPL=80.85è¯´æ˜25%çš„å‰ªæç‡å¤ªé«˜äº†ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šé™ä½åˆ°15-20%

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_pruned_15pct \
    --pruning_ratio 0.15 \
    --importance_method removal \
    --importance_samples 50 \
    --pruning_strategy inverse \
    --prune_mlp \
    --save_model \
    --test_after_prune
```

**å…³é”®å‚æ•°**ï¼š
- `--pruning_ratio 0.15` - é™ä½åˆ°15%ï¼ˆä»25%ï¼‰
- `--test_after_prune` - ç«‹å³æ£€æŸ¥PPL

**é¢„æœŸå‰ªæåPPL**ï¼šåº”è¯¥åœ¨ 10-12 ä¹‹é—´ã€‚

---

### æ­¥éª¤3ï¼šå¦‚æœä»ç„¶PPLè¿‡é«˜ï¼Œè¿›ä¸€æ­¥è°ƒæ•´

#### 3.1 åªå‰ªæAttentionï¼Œä¸å‰ªæMLP

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_attn_only_20pct \
    --pruning_ratio 0.20 \
    --importance_method removal \
    --pruning_strategy inverse \
    --save_model \
    --test_after_prune
```

**æ³¨æ„**ï¼šç§»é™¤äº† `--prune_mlp`

#### 3.2 ä¿æŠ¤é¦–å°¾å±‚

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_middle_layers \
    --pruning_ratio 0.20 \
    --layer_start 3 \
    --layer_end 29 \
    --importance_method removal \
    --pruning_strategy inverse \
    --prune_mlp \
    --save_model \
    --test_after_prune
```

**è¯´æ˜**ï¼šè·³è¿‡å‰3å±‚å’Œå3å±‚ï¼ˆè¿™äº›å±‚é€šå¸¸æ›´é‡è¦ï¼‰

#### 3.3 ä½¿ç”¨æ›´ä¿å®ˆçš„å‰ªæç­–ç•¥

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_conservative \
    --pruning_ratio 0.20 \
    --importance_method removal \
    --pruning_strategy inverse \
    --alpha 1.5 \
    --min_pruning_rate 0.10 \
    --max_pruning_rate 0.35 \
    --prune_mlp \
    --save_model \
    --test_after_prune
```

**å…³é”®å‚æ•°**ï¼š
- `--alpha 1.5` - å¢åŠ é‡è¦æ€§æƒé‡ï¼ˆæ›´ä¿å®ˆï¼‰
- `--min_pruning_rate 0.10` - æœ€å°‘å‰ª10%
- `--max_pruning_rate 0.35` - æœ€å¤šå‰ª35%

---

### æ­¥éª¤4ï¼šç¡®è®¤å‰ªææˆåŠŸåå†å¾®è°ƒ

**æ£€æŸ¥ç‚¹**ï¼š
- âœ… å‰ªæåPPL < 15
- âœ… GQAæ¯”ä¾‹ä¿æŒ4:1
- âœ… æ— é”™è¯¯ä¿¡æ¯

ç„¶åè¿›è¡Œå¾®è°ƒï¼š

#### 4.1 ä½¿ç”¨æä½å­¦ä¹ ç‡å¾®è°ƒ

```bash
python test_finetuning.py \
    --model_path prune_log/llama3_pruned_15pct/pytorch_model.bin \
    --save_name finetune_ultra_safe \
    --lr 5e-7 \
    --samples 500 \
    --epochs 2 \
    --seq_len 256 \
    --grad_accum 4 \
    --max_grad_norm 0.5 \
    --warmup_steps 20 \
    --test_before \
    --test_after
```

**å…³é”®å‚æ•°**ï¼š
- `--lr 5e-7` - æä½å­¦ä¹ ç‡ï¼ˆæ¯”é»˜è®¤çš„1e-5ä½20å€ï¼‰
- `--max_grad_norm 0.5` - å¼ºæ¢¯åº¦è£å‰ª
- `--seq_len 256` - çŸ­åºåˆ—ï¼ˆå‡å°‘æ˜¾å­˜å‹åŠ›ï¼‰

#### 4.2 å®Œæ•´æµç¨‹ï¼ˆå‰ªæ+å¾®è°ƒï¼‰

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_complete_safe \
    --pruning_ratio 0.15 \
    --importance_method removal \
    --pruning_strategy inverse \
    --prune_mlp \
    --save_model \
    --test_after_prune \
    --finetune \
    --finetune_method full \
    --finetune_lr 5e-7 \
    --finetune_epochs 2 \
    --finetune_samples 1000 \
    --finetune_grad_accum 4 \
    --finetune_max_grad_norm 0.5 \
    --finetune_warmup_steps 50
```

---

## ğŸ“Š PPLåŸºå‡†å‚è€ƒ

| å‰ªæç‡ | å‰ªæåPPLï¼ˆæ­£å¸¸ï¼‰ | å‰ªæåPPLï¼ˆå¼‚å¸¸ï¼‰ | æ˜¯å¦èƒ½å¾®è°ƒ |
|-------|----------------|-----------------|----------|
| 15% | 10-11 | >30 | âœ… å¯ä»¥ |
| 20% | 11-13 | >40 | âœ… å¯ä»¥ |
| 25% | 12-15 | >50 | âš ï¸  éœ€è¦é™ä½å­¦ä¹ ç‡ |
| 30% | 14-18 | >70 | âŒ å»ºè®®é‡æ–°å‰ªæ |
| >30% | 16-25 | >100 | âŒ å¿…é¡»é‡æ–°å‰ªæ |

**æ‚¨çš„æƒ…å†µ**ï¼š25%å‰ªæåPPL=80.85 â†’ **ä¸¥é‡å¼‚å¸¸**ï¼Œå¿…é¡»é‡æ–°å‰ªæã€‚

---

## ğŸ”§ æ–°å¢çš„å®‰å…¨æ£€æŸ¥

ç°åœ¨å¾®è°ƒè„šæœ¬åŒ…å«ä»¥ä¸‹å®‰å…¨æœºåˆ¶ï¼š

### 1. å¾®è°ƒå‰æ¨¡å‹å¥åº·æ£€æŸ¥

```
æ£€æŸ¥æ¨¡å‹æƒé‡å¥åº·çŠ¶æ€...
âœ… æ¨¡å‹æƒé‡æ­£å¸¸
```

å¦‚æœæ£€æµ‹åˆ°é—®é¢˜ï¼š
```
âŒ æ¨¡å‹å­˜åœ¨æ•°å€¼é—®é¢˜:
  NaNå‚æ•°æ•°é‡: 1,234
  Infå‚æ•°æ•°é‡: 567

å»ºè®®:
  1. é‡æ–°è¿è¡Œå‰ªææµç¨‹
  2. æ£€æŸ¥å‰ªæç‡æ˜¯å¦è¿‡é«˜
  3. è¿è¡Œè¯Šæ–­è„šæœ¬: python diagnose_model.py --model_path <path>
```

### 2. è®­ç»ƒä¸­Lossç›‘æ§

æ¯ä¸ªbatchéƒ½ä¼šæ£€æŸ¥Lossï¼š
```python
if torch.isnan(loss) or torch.isinf(loss) or loss_value > 1e6:
    # ç«‹å³åœæ­¢å¹¶ç»™å‡ºè¯¦ç»†å»ºè®®
```

---

## ğŸ¯ æ¨èçš„å®Œæ•´æµç¨‹

### æ–¹æ¡ˆAï¼šä¿å®ˆå‰ªæï¼ˆæ¨èï¼‰â­

```bash
# æ­¥éª¤1ï¼š15%å‰ªæ
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_safe_15pct \
    --pruning_ratio 0.15 \
    --importance_method removal \
    --pruning_strategy inverse \
    --alpha 1.2 \
    --prune_mlp \
    --save_model \
    --test_after_prune

# æ­¥éª¤2ï¼šæ£€æŸ¥PPLï¼ˆåº”è¯¥<12ï¼‰
grep "å‰ªæå PPL" prune_log/llama3_safe_15pct/*/training.log

# æ­¥éª¤3ï¼šå¦‚æœPPLæ­£å¸¸ï¼Œè¿›è¡Œå¾®è°ƒ
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_safe_15pct_finetuned \
    --pruning_ratio 0.15 \
    --skip_importance_analysis \
    --importance_config prune_log/llama3_safe_15pct/layer_importance_config.json \
    --prune_mlp \
    --save_model \
    --test_after_prune \
    --finetune \
    --finetune_lr 5e-7 \
    --finetune_epochs 2 \
    --finetune_samples 1000 \
    --finetune_grad_accum 4 \
    --finetune_max_grad_norm 0.5
```

### æ–¹æ¡ˆBï¼šåªå‰ªAttentionï¼ˆæ›´å®‰å…¨ï¼‰

```bash
python llama3_unbalanced_pruning_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name llama3_attn_20pct \
    --pruning_ratio 0.20 \
    --importance_method removal \
    --pruning_strategy inverse \
    --save_model \
    --test_after_prune \
    --finetune \
    --finetune_lr 1e-6 \
    --finetune_epochs 2 \
    --finetune_samples 1000
```

**æ³¨æ„**ï¼šæ²¡æœ‰ `--prune_mlp`

---

## ğŸ“ æˆåŠŸæ ‡å¿—

### å‰ªææˆåŠŸï¼š
```
å‰ªæå PPL:   wikitext2 (wikitext-2-raw-v1): 11.23
GQAæ¯”ä¾‹éªŒè¯: âœ… æ‰€æœ‰å±‚ä¿æŒ4:1
å®é™…å‰ªæç‡: 15.12%
```

### å¾®è°ƒæˆåŠŸï¼š
```
æ£€æŸ¥æ¨¡å‹æƒé‡å¥åº·çŠ¶æ€...
âœ… æ¨¡å‹æƒé‡æ­£å¸¸

å¼€å§‹ç¬¬ 1/2 è½®å¾®è°ƒ...
  è¿›åº¦: 10% | å¹³å‡Loss: 2.1234 | LR: 2.50e-07
  è¿›åº¦: 20% | å¹³å‡Loss: 1.9876 | LR: 5.00e-07
  ...
  è¿›åº¦: 100% | å¹³å‡Loss: 1.6543 | LR: 4.75e-07
âœ… ç¬¬ 1 è½®å®Œæˆï¼Œå¹³å‡Loss: 1.6543

å¾®è°ƒå PPL:   wikitext2 (wikitext-2-raw-v1): 10.87
```

**å…³é”®æŒ‡æ ‡**ï¼š
- âœ… Lossä»~2.0ä¸‹é™åˆ°~1.6ï¼ˆæ­£å¸¸ï¼‰
- âœ… æ— NaNæˆ–Inf
- âœ… PPLä¸‹é™ï¼ˆä»11.23åˆ°10.87ï¼‰

---

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„PPLè¿™ä¹ˆé«˜ï¼ˆ80.85ï¼‰ï¼Ÿ

**A**: å¯èƒ½çš„åŸå› ï¼š
1. å‰ªæç‡25%å¯¹æ‚¨çš„æ¨¡å‹æ¥è¯´å¤ªé«˜
2. æŸäº›å…³é”®å±‚è¢«è¿‡åº¦å‰ªæ
3. MLPå‰ªæå¯èƒ½è¿‡äºæ¿€è¿›

**è§£å†³**ï¼šé™ä½åˆ°15-20%ï¼Œå…ˆä¸å‰ªMLP

### Q2: é™ä½å­¦ä¹ ç‡åè¿˜æ˜¯NaNæ€ä¹ˆåŠï¼Ÿ

**A**: è¯´æ˜å‰ªæåçš„æ¨¡å‹å·²ç»æ— æ³•æ¢å¤ï¼Œå¿…é¡»ï¼š
1. é‡æ–°å‰ªæï¼Œä½¿ç”¨æ›´ä½çš„å‰ªæç‡ï¼ˆ10-15%ï¼‰
2. è·³è¿‡é—®é¢˜å±‚ï¼ˆä½¿ç”¨`--layer_start`å’Œ`--layer_end`ï¼‰
3. åªå‰ªAttentionï¼Œä¸å‰ªMLP

### Q3: å¦‚ä½•åˆ¤æ–­å‰ªæç‡æ˜¯å¦åˆé€‚ï¼Ÿ

**A**: çœ‹å‰ªæåçš„PPLï¼š
- PPL < 15: âœ… å¾ˆå¥½ï¼Œå¯ä»¥ç»§ç»­
- PPL 15-25: âš ï¸  å¯æ¥å—ï¼Œä½†è¦å°å¿ƒå¾®è°ƒ
- PPL 25-50: âŒ å¤ªé«˜äº†ï¼Œé™ä½å‰ªæç‡
- PPL > 50: âŒ å®Œå…¨æŸåï¼Œå¿…é¡»é‡æ–°å‰ªæ

### Q4: å¾®è°ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**A**: å–å†³äºé…ç½®ï¼š
- 500æ ·æœ¬ï¼Œ1è½®ï¼š~30åˆ†é’Ÿ
- 1000æ ·æœ¬ï¼Œ2è½®ï¼š~1-1.5å°æ—¶
- 2000æ ·æœ¬ï¼Œ3è½®ï¼š~2-3å°æ—¶

---

## ğŸ”§ è°ƒè¯•æŠ€å·§

### 1. å®æ—¶ç›‘æ§Loss

```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯
tail -f prune_log/llama3_*/*/training.log | grep "Loss"
```

### 2. æ£€æŸ¥å‰ªæç»Ÿè®¡

```bash
grep -E "(å‰ªæç‡|PPL|GQA)" prune_log/llama3_*/*/training.log
```

### 3. å¯¹æ¯”ä¸åŒé…ç½®

```bash
# è¿è¡Œå¤šä¸ªé…ç½®
for ratio in 0.15 0.20 0.25; do
    python llama3_unbalanced_pruning_gqa_aware.py \
        --pruning_ratio $ratio \
        --test_after_prune \
        --save_ckpt_log_name test_ratio_${ratio}
done

# å¯¹æ¯”ç»“æœ
grep "å‰ªæå PPL" prune_log/test_ratio_*/*/training.log
```

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ— æ³•è§£å†³ï¼Œè¯·æä¾›ï¼š
1. å®Œæ•´çš„å‰ªææ—¥å¿—
2. `diagnose_model.py`çš„è¾“å‡º
3. å‰ªæåçš„PPLå€¼
4. ä½¿ç”¨çš„å…·ä½“å‘½ä»¤

---

**æœ€åæ›´æ–°**: 2025-11-17
**ç‰ˆæœ¬**: 1.1
