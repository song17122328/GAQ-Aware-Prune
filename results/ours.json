{
  "model_path": "prune_log/ppl_search_20251118_005448_ratio_0.7_9.3_freeze_8/pytorch_model.bin",
  "timestamp": "2025-11-19T22:28:46.608587",
  "metrics": {
    "model_info": {
      "total_params": 6307975168,
      "trainable_params": 1268789248,
      "total_params_M": 6307.975168,
      "total_params_B": 6.307975168,
      "attention_params": 1216348160,
      "mlp_params": 4040687616,
      "attention_params_M": 1216.34816,
      "mlp_params_M": 4040.687616,
      "attention_ratio": 0.1928270368232369,
      "mlp_ratio": 0.6405680917227098,
      "num_layers": 32,
      "model_size_mb": 12031.507934570312,
      "model_size_gb": 11.74951946735382
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.6700305810397553,
        "correct": 2191,
        "total": 3270
      },
      "piqa": {
        "accuracy": 0.6931447225244831,
        "correct": 1274,
        "total": 1838
      },
      "hellaswag": {
        "accuracy": 0.35002987452698664,
        "correct": 3515,
        "total": 10042
      },
      "winogrande": {
        "accuracy": 0.5769534333070244,
        "correct": 731,
        "total": 1267
      },
      "arc_easy": {
        "accuracy": 0.656140350877193,
        "correct": 374,
        "total": 570
      },
      "arc_challenge": {
        "accuracy": 0.3177257525083612,
        "correct": 95,
        "total": 299
      },
      "openbookqa": {
        "accuracy": 0.24,
        "correct": 120,
        "total": 500
      }
    },
    "avg_zeroshot_acc": 0.5005749592548291
  }
}