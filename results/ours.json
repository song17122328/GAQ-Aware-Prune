{
  "model_path": "prune_log/ppl_search_20251118_005448_ratio_0.7_9.3_freeze_8/pytorch_model.bin",
  "timestamp": "2025-11-19T21:44:45.869069",
  "metrics": {
    "model_info": {
      "total_params": 6307975168,
      "trainable_params": 1268789248,
      "total_params_M": 6307.975168,
      "total_params_B": 6.307975168,
      "attention_params": 1216348160,
      "mlp_params": 4040687616,
      "attention_params_M": 1216.34816,
      "mlp_params_M": 4040.687616,
      "attention_ratio": 0.1928270368232369,
      "mlp_ratio": 0.6405680917227098,
      "num_layers": 32,
      "model_size_mb": 12031.507934570312,
      "model_size_gb": 11.74951946735382
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.6217125382262997,
        "correct": 2033,
        "total": 3270
      },
      "piqa": {
        "accuracy": 0.6898803046789989,
        "correct": 1268,
        "total": 1838
      },
      "hellaswag": {
        "accuracy": 0.34734116709818763,
        "correct": 3488,
        "total": 10042
      },
      "winogrande": {
        "accuracy": 0.5043409629044988,
        "correct": 639,
        "total": 1267
      },
      "arc_easy": {
        "accuracy": 0.45263157894736844,
        "correct": 258,
        "total": 570
      },
      "arc_challenge": {
        "accuracy": 0.30434782608695654,
        "correct": 91,
        "total": 299
      },
      "openbookqa": {
        "accuracy": 0.192,
        "correct": 96,
        "total": 500
      }
    },
    "avg_zeroshot_acc": 0.44460776827747284
  }
}