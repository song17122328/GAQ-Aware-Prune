# GAQ-Aware-Prune é¡¹ç›®å…¨é¢æ€»ç»“

**æ–‡æ¡£ç›®çš„**: ä¸ºé¡¹ç›®é‡æ„å’Œè¿›ä¸€æ­¥å‘å±•æä¾›æŠ€æœ¯æ€»ç»“å’Œæ–¹å‘æŒ‡å¯¼

**åˆ›å»ºæ—¶é—´**: 2025-11-21

**ç‰ˆæœ¬**: 1.0

---

## ç›®å½•

1. [é¡¹ç›®æ¼”è¿›å†ç¨‹](#1-é¡¹ç›®æ¼”è¿›å†ç¨‹)
2. [æ ¸å¿ƒæŠ€æœ¯æ¡†æ¶](#2-æ ¸å¿ƒæŠ€æœ¯æ¡†æ¶)
3. [å½“å‰æ¶æ„åˆ†æ](#3-å½“å‰æ¶æ„åˆ†æ)
4. [è¯„ä¼°æŒ‡æ ‡ä¸Baseline](#4-è¯„ä¼°æŒ‡æ ‡ä¸baseline)
5. [é‡æ„æ–¹å‘å»ºè®®](#5-é‡æ„æ–¹å‘å»ºè®®)

---

## 1. é¡¹ç›®æ¼”è¿›å†ç¨‹

### 1.1 é˜¶æ®µä¸€ï¼šåŸºç¡€GQAæ„ŸçŸ¥å‰ªæï¼ˆv0.1ï¼‰

**æ—¶é—´èŠ‚ç‚¹**: é¡¹ç›®åˆæœŸ

**æ ¸å¿ƒæœºåˆ¶**:
- **GQAç»“æ„ä¿æŒ**: ä¸¥æ ¼ç»´æŠ¤ 4:1 çš„ Q:KV head æ¯”ä¾‹
- **Tayloré‡è¦æ€§**: ä½¿ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€ `Importance = |Î¸ Â· âˆ‡L|` è¯„ä¼°ç¥ç»å…ƒé‡è¦æ€§
- **é€å±‚ç‹¬ç«‹å‰ªæ**: æ¯å±‚ç‹¬ç«‹è®¡ç®—é‡è¦æ€§å¹¶æ‰§è¡Œå‰ªæ

**å…³é”®å®ç°**:
```python
# core/methods/gqa_aware.py
def compute_gqa_group_importance(layer, head_dim=128, gqa_ratio=4):
    """
    è®¡ç®—æ¯ä¸ªGQAç»„çš„é‡è¦æ€§ï¼ˆ1ä¸ªKV head + 4ä¸ªQ headsï¼‰

    æ ¸å¿ƒå…¬å¼: I_group = Î£|weight Ã— gradient|
    """
    # ä¸ºæ¯ä¸ªKV headèšåˆå¯¹åº”çš„Q headsé‡è¦æ€§
    # ç¡®ä¿å‰ªææ—¶ä¿æŒ4:1æ¯”ä¾‹
```

**å±€é™æ€§**:
- âŒ å±‚ä¸å±‚ä¹‹é—´æ— æ³•å¯¹æ¯”ï¼ˆæ— æ³•åˆ¤æ–­layer 5çš„æŸä¸ªheadæ˜¯å¦æ¯”layer 10çš„æ›´é‡è¦ï¼‰
- âŒ Attentionå’ŒMLPç»„ä»¶æ— æ³•å¯¹æ¯”ï¼ˆæ— æ³•å…¨å±€æƒè¡¡ï¼‰
- âŒ å‰ªæç‡å›ºå®šåˆ†é…ï¼Œç¼ºä¹çµæ´»æ€§

---

### 1.2 é˜¶æ®µäºŒï¼šéå‡è¡¡å±‚çº§å‰ªæ + åˆ†å¸ƒæœç´¢ï¼ˆv1.0ï¼‰

**æ—¶é—´èŠ‚ç‚¹**: ä¸­æœŸå‘å±•

**æ ¸å¿ƒæ”¹è¿›**:

#### 1.2.1 å±‚é‡è¦æ€§è¯„ä¼°

å¼•å…¥**å±‚çº§é‡è¦æ€§åˆ†æ**ï¼Œè§£å†³"ä¸åŒå±‚åº”è¯¥å‰ªå¤šå°‘"çš„é—®é¢˜ã€‚

**æ–¹æ³•ä¸€ï¼šRemoval-basedï¼ˆç§»é™¤æ³•ï¼‰**
```python
# core/importance/layer_analyzer.py
def measure_layer_importance_by_removal(model, texts):
    """
    é€å±‚ç§»é™¤å¹¶æµ‹é‡PPLå˜åŒ–

    åŸç†: å¦‚æœç§»é™¤æŸå±‚åPPLä¸Šå‡å¾ˆå¤šï¼Œè¯´æ˜è¯¥å±‚å¾ˆé‡è¦
    """
    baseline_ppl = evaluate_ppl(model, texts)

    for layer_idx in range(num_layers):
        # ä¸´æ—¶ç¦ç”¨è¯¥å±‚
        with DisableLayer(model, layer_idx):
            layer_ppl = evaluate_ppl(model, texts)

        # é‡è¦æ€§ = PPLå¢é‡
        importance[layer_idx] = layer_ppl - baseline_ppl
```

**æ–¹æ³•äºŒï¼šActivation-basedï¼ˆæ¿€æ´»æ³•ï¼‰**
```python
def measure_layer_importance_by_activation(model, texts):
    """
    åŸºäºæ¿€æ´»å€¼ç»Ÿè®¡

    åŸç†: æ¿€æ´»å€¼å˜åŒ–å¤§çš„å±‚æ›´é‡è¦
    """
    # æ”¶é›†æ¯å±‚çš„æ¿€æ´»ç»Ÿè®¡é‡ï¼ˆå‡å€¼ã€æ–¹å·®ç­‰ï¼‰
    # æ¿€æ´»å€¼å˜åŒ–å¤§ â†’ ä¿¡æ¯ä¼ é€’å¤š â†’ é‡è¦æ€§é«˜
```

**å±‚é‡è¦æ€§åˆ†å¸ƒè§‚å¯Ÿ**:
```
å±‚é‡è¦æ€§å‘ˆç°"Uå‹"åˆ†å¸ƒ:
  é‡è¦æ€§
    â†‘
    |     â–ˆ           â–ˆ
    |    â–ˆ â–ˆ         â–ˆ â–ˆ
    |   â–ˆ   â–ˆ       â–ˆ   â–ˆ
    |  â–ˆ     â–ˆ     â–ˆ     â–ˆ
    | â–ˆ       â–ˆ   â–ˆ       â–ˆ
    |â–ˆ         â–ˆ â–ˆ         â–ˆ
    +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ å±‚ç´¢å¼•
     0    8   16   24    31
    é¦–å±‚           ä¸­é—´å±‚        å°¾å±‚
   ï¼ˆç‰¹å¾          ï¼ˆå¯å‹ç¼©ï¼‰    ï¼ˆè¾“å‡º
    æå–ï¼‰                       è§£ç ï¼‰
```

**å‘ç°**: é¦–å±‚å’Œå°¾å±‚æœ€é‡è¦ï¼Œä¸­é—´å±‚å­˜åœ¨å†—ä½™

#### 1.2.2 éå‡è¡¡å‰ªæç­–ç•¥

**æ ¸å¿ƒæ€æƒ³**: é‡è¦çš„å±‚å°‘å‰ªï¼Œä¸é‡è¦çš„å±‚å¤šå‰ª

**å®ç°**:
```python
# core/importance/layer_analyzer.py
class UnbalancedStructuredPruningCalculator:
    def compute_layer_pruning_rates(self,
                                   target_overall_rate,
                                   strategy='inverse',
                                   alpha=1.0):
        """
        æ ¹æ®å±‚é‡è¦æ€§è®¡ç®—æ¯å±‚çš„å‰ªæç‡

        Args:
            target_overall_rate: æ€»ä½“ç›®æ ‡å‰ªæç‡ï¼ˆå¦‚0.25ï¼‰
            strategy:
                - 'inverse': é‡è¦å±‚å‰ªå°‘ï¼ˆæ¨èï¼‰
                - 'proportional': é‡è¦å±‚å‰ªå¤š
                - 'uniform': å‡åŒ€å‰ªæ
            alpha: å±‚é—´å·®å¼‚ç³»æ•°ï¼ˆ0.5-3.0ï¼‰

        Returns:
            {layer_idx: pruning_rate}
        """
        if strategy == 'inverse':
            # å½’ä¸€åŒ–é‡è¦æ€§
            norm_importance = importance / sum(importance)

            # åå‘æƒé‡: é‡è¦æ€§ä½çš„å±‚å‰ªæç‡é«˜
            weights = 1.0 / (norm_importance + epsilon) ** alpha

            # å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿æ€»å‰ªæç‡è¾¾æ ‡
            layer_rates = weights / sum(weights) * target_overall_rate * num_layers
```

**ç¤ºä¾‹**:
```
å‡è®¾ç›®æ ‡æ€»å‰ªæç‡ = 25%ï¼Œ3å±‚æ¨¡å‹

å±‚é‡è¦æ€§:    [0.8,  0.3,  0.7]  (å½’ä¸€åŒ–å)
åå‘æƒé‡:    [1.25, 3.33, 1.43]
å‰ªæç‡åˆ†é…:  [15%,  40%,  20%]  (å¹³å‡25%)

ç»“æœ: Layer 1ï¼ˆé‡è¦æ€§0.3ï¼‰å‰ªææœ€å¤šï¼ˆ40%ï¼‰ï¼ŒLayer 0ï¼ˆé‡è¦æ€§0.8ï¼‰å‰ªææœ€å°‘ï¼ˆ15%ï¼‰
```

#### 1.2.3 Attention:MLPå‰ªæåˆ†å¸ƒæ§åˆ¶

**é—®é¢˜**: LLaMA-3-8Bä¸­ï¼ŒAttentionå’ŒMLPçš„å‚æ•°é‡å·®å¼‚å·¨å¤§
- Attention: ~19.2% çš„æ¨¡å‹å‚æ•°
- MLP: ~80.8% çš„æ¨¡å‹å‚æ•°

å¦‚æœå‡åŒ€å‰ªæ25%ï¼Œä¼šå¯¼è‡´ï¼š
- Attentionå‰ªæ‰25% â†’ å½±å“æ³¨æ„åŠ›æœºåˆ¶
- MLPä¹Ÿå‰ªæ‰25% â†’ ä½†MLPå‚æ•°å¤šå¾—å¤šï¼ŒæŸå¤±æ›´å¤§

**è§£å†³æ–¹æ¡ˆ**: `--pruning_distribution x:y` å‚æ•°

```python
# llama3_unbalanced_pruning_gqa_aware.py
def allocate_pruning_budget(pruning_ratio, distribution, attn_params, mlp_params):
    """
    æ ¹æ®distributionæ¯”ä¾‹åˆ†é…å‰ªæé¢„ç®—

    Args:
        pruning_ratio: æ€»å‰ªæç‡ï¼ˆå¦‚0.25ï¼‰
        distribution: "x:y"æ ¼å¼ï¼Œx+y=10ï¼ˆå¦‚"2:8"ï¼‰
        attn_params: Attentionæ€»å‚æ•°é‡
        mlp_params: MLPæ€»å‚æ•°é‡

    ç¤ºä¾‹:
        pruning_ratio = 0.25
        distribution = "2:8"

        æ€»å‰ªæé‡ = (attn_params + mlp_params) * 0.25

        Attentionå‰ªæé‡ = æ€»å‰ªæé‡ * (2/10) = æ€»å‰ªæé‡ * 20%
        MLPå‰ªæé‡ = æ€»å‰ªæé‡ * (8/10) = æ€»å‰ªæé‡ * 80%
    """
    x, y = parse_distribution(distribution)  # "2:8" â†’ (2.0, 8.0)

    total_prunable = attn_params + mlp_params
    total_prune = total_prunable * pruning_ratio

    attn_prune = total_prune * (x / (x + y))
    mlp_prune = total_prune * (y / (x + y))

    # è½¬æ¢ä¸ºå„è‡ªçš„å‰ªæç‡
    attn_rate = attn_prune / attn_params
    mlp_rate = mlp_prune / mlp_params

    return attn_rate, mlp_rate
```

**å…¸å‹é…ç½®å¯¹æ¯”**:

| é…ç½® | Attentionå‰ªæ | MLPå‰ªæ | è¯´æ˜ |
|------|--------------|---------|------|
| `5:5` | 32.6% | 19.3% | å‡è¡¡åˆ†é…ï¼ˆä½†Attentionå‰ªæ›´å¤šï¼‰ |
| `2:8` | 26.0% | 24.8% | **æ¥è¿‘ç­‰å‰ªæç‡**ï¼ˆæ¨èï¼‰ |
| `0:10` | 0% | 24.7% | åªå‰ªMLPï¼Œä¿æŠ¤Attention |
| `10:0` | 65.1% | 0% | åªå‰ªAttentionï¼Œä¿æŠ¤MLP |

**å…³é”®æ´å¯Ÿ**: `2:8` é…ç½®ä¸‹ï¼ŒAttentionå’ŒMLPçš„**å®é™…å‰ªæç‡**æ¥è¿‘ï¼Œæ›´åŠ å¹³è¡¡ï¼

#### 1.2.4 è‡ªåŠ¨åˆ†å¸ƒæœç´¢

**é—®é¢˜**: å¦‚ä½•æ‰¾åˆ°æœ€ä¼˜çš„ `x:y` æ¯”ä¾‹ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**: `search_optimal_distribution.py` - æ™ºèƒ½ä¸¤é˜¶æ®µæœç´¢

```python
class PPLSearcher:
    """
    ä¸¤é˜¶æ®µæœç´¢ç­–ç•¥:

    é˜¶æ®µ1: ç²—ç²’åº¦æœç´¢ï¼ˆæ­¥é•¿=1ï¼‰
        - ä»æ™ºèƒ½èµ·ç‚¹ï¼ˆ2:8ï¼‰åŒå‘æœç´¢
        - å‘å·¦: 1:9, 0:10
        - å‘å³: 3:7, 4:6, 5:5, ..., 10:0
        - æ—©åœæœºåˆ¶: æ£€æµ‹åˆ°PPLæŒç»­ä¸Šå‡åˆ™åœæ­¢

    é˜¶æ®µ2: ç»†ç²’åº¦æœç´¢ï¼ˆæ­¥é•¿=0.1ï¼‰
        - åœ¨æœ€ä¼˜ç‚¹é™„è¿‘ç²¾ç»†åŒ–æœç´¢
        - å¦‚: æœ€ä¼˜ç‚¹æ˜¯2:8ï¼Œåˆ™æµ‹è¯•1.9:8.1, 2.1:7.9ç­‰
    """

    def bidirectional_search(self, start_ratio, step=1.0):
        """
        åŒå‘æœç´¢ + æ—©åœ

        æ—©åœæ¡ä»¶: è¿ç»­3æ¬¡PPLä¸Šå‡ä¸”åŠ é€Ÿï¼ˆäºŒé˜¶å¯¼æ•°>0ï¼‰
        """
        # å‘å·¦æœç´¢
        for ratio in [start-step, start-2*step, ...]:
            ppl = self.run_pruning(ratio)
            if self._should_stop(ppl_history):
                break

        # å‘å³æœç´¢
        for ratio in [start+step, start+2*step, ...]:
            ppl = self.run_pruning(ratio)
            if self._should_stop(ppl_history):
                break
```

**æœç´¢æ•ˆç‡**:
- å¯èƒ½æµ‹è¯•æ•°: 11 (ç²—) + 10 (ç»†) = 21æ¬¡
- å®é™…æµ‹è¯•æ•°: ~9 (ç²—) + 6 (ç»†) = 15æ¬¡ï¼ˆèŠ‚çœ30%ï¼‰

**å®éªŒå‘ç°**:
```
å…¸å‹æœç´¢ç»“æœï¼ˆLLaMA-3-8Bï¼Œå‰ªæç‡25%ï¼‰:

åˆ†å¸ƒ      PPL      æ’å
0.0:10.0  46.87    4
0.1:9.9   46.23    3
0.2:9.8   45.89    2
0.3:9.7   45.12    ğŸ† æœ€ä¼˜
0.4:9.6   47.23    5
...
2.0:8.0   83.77    10
5.0:5.0   142.35   15

ç»“è®º: æåº¦åå‘MLPå‰ªæï¼ˆ0.3:9.7ï¼‰æ•ˆæœæœ€å¥½ï¼
```

#### 1.2.5 å±‚å†»ç»“æœºåˆ¶

**åŠ¨æœº**: å³ä½¿é‡‡ç”¨éå‡è¡¡ç­–ç•¥ï¼Œæœ€é‡è¦çš„å‡ å±‚ä¹Ÿå¯èƒ½è¢«è½»åº¦å‰ªæï¼Œå½±å“æ€§èƒ½

**è§£å†³æ–¹æ¡ˆ**: `--freeze_top_n_layers N`

```python
def apply_layer_freezing(layer_importance, freeze_top_n):
    """
    å†»ç»“æœ€é‡è¦çš„Nå±‚ï¼Œå®Œå…¨ä¸å‚ä¸å‰ªæ

    å…¶ä»–å±‚æ‰¿æ‹…å…¨éƒ¨å‰ªæä»»åŠ¡ï¼ˆå‰ªæç‡ä¼šç›¸åº”æé«˜ï¼‰
    """
    # æŒ‰é‡è¦æ€§æ’åº
    sorted_layers = sorted(layer_importance.items(),
                          key=lambda x: x[1],
                          reverse=True)

    # æ ‡è®°å‰Nå±‚ä¸º"å†»ç»“"
    frozen_layers = [idx for idx, _ in sorted_layers[:freeze_top_n]]

    # é‡æ–°åˆ†é…å‰ªæç‡åˆ°æœªå†»ç»“çš„å±‚
    active_layers = [idx for idx in range(num_layers)
                    if idx not in frozen_layers]

    # åœ¨active_layersä¸­é‡æ–°è®¡ç®—å‰ªæç‡
    # æ€»å‰ªæé‡ä¸å˜ï¼Œä½†åˆ†é…åˆ°æ›´å°‘çš„å±‚
```

**æ•ˆæœ**:
```
ä¸å†»ç»“ï¼ˆ32å±‚å‡å‚ä¸ï¼‰:
  Layer 0:  15% (é‡è¦)
  Layer 1:  18%
  ...
  Layer 31: 16% (é‡è¦)

å†»ç»“å‰3å±‚ï¼ˆ29å±‚å‚ä¸ï¼‰:
  Layer 0:  0%  â† å†»ç»“
  Layer 1:  0%  â† å†»ç»“
  Layer 2:  0%  â† å†»ç»“
  Layer 3:  20% â† å‰ªæç‡æé«˜ï¼ˆå› ä¸ºæ€»é‡ä¸å˜ï¼Œå±‚æ•°å‡å°‘ï¼‰
  ...
  Layer 31: 18%
```

**å…¸å‹é…ç½®**: `--freeze_top_n_layers 3` æˆ– `5`

---

### 1.3 é˜¶æ®µä¸‰ï¼šå…¨å±€å‰ªææ¡†æ¶ï¼ˆv2.0 - å½“å‰æœ€å…ˆè¿›ï¼‰

**æ—¶é—´èŠ‚ç‚¹**: è¿‘æœŸï¼ˆæœ€æ–°æ¶æ„ï¼‰

**æ ¸å¿ƒçªç ´**: ä»"é€å±‚ä¼˜åŒ–"å‡çº§åˆ°"å…¨å±€ä¼˜åŒ–"

#### 1.3.1 ç†è®ºåŸºç¡€ï¼šåˆ†æ•°èƒŒåŒ…é—®é¢˜

**é—®é¢˜å»ºæ¨¡**:
```
ç»™å®š:
  - æ¨¡å‹ä¸­çš„æ‰€æœ‰å‰ªæå•å…ƒ U = {uâ‚, uâ‚‚, ..., uâ‚™}
  - æ¯ä¸ªå•å…ƒçš„é‡è¦æ€§ I(u) å’Œæˆæœ¬ C(u)
  - æ€»å‚æ•°é¢„ç®—çº¦æŸ B

ç›®æ ‡:
  é€‰æ‹©ä¿ç•™å“ªäº›å•å…ƒï¼Œä½¿å¾—æ€»é‡è¦æ€§æœ€å¤§

  max Î£ I(uáµ¢) Â· xáµ¢
  s.t. Î£ C(uáµ¢) Â· xáµ¢ â‰¤ B
       xáµ¢ âˆˆ {0, 1}

ç­‰ä»·äº:
  æœ€å°åŒ–å‰ªææŸå¤± = å‰ªæ‰å•å…ƒçš„æ€»é‡è¦æ€§
```

**è´ªå¿ƒæ±‚è§£**:
```python
# å…³é”®æ´å¯Ÿ: æŒ‰"æ€§ä»·æ¯”"æ’åº
# ä¼˜å…ˆå‰ªæ‰ Score = I/C æœ€ä½çš„å•å…ƒ

def fractional_knapsack_pruning(units, budget):
    """
    åˆ†æ•°èƒŒåŒ…å‰ªæç®—æ³•

    1. è®¡ç®—æ¯ä¸ªå•å…ƒçš„æ€§ä»·æ¯” Score = Importance / Cost
    2. æŒ‰Scoreå‡åºæ’åºï¼ˆæœ€ä½çš„ä¼˜å…ˆå‰ªï¼‰
    3. ç´¯åŠ å‚æ•°é‡ç›´åˆ°è¾¾åˆ°é¢„ç®—
    """
    # è®¡ç®—æ€§ä»·æ¯”
    for u in units:
        u.score = u.importance / u.cost

    # æŒ‰scoreå‡åºæ’åº
    units.sort(key=lambda u: u.score)

    # è´ªå¿ƒé€‰æ‹©
    pruned = []
    total_cost = 0

    for u in units:
        if total_cost + u.cost <= budget:
            pruned.append(u)
            total_cost += u.cost
        else:
            break  # é¢„ç®—ç”¨å°½

    return pruned
```

#### 1.3.2 å…¨å±€åˆ†æè¡¨æ„å»º

**æ ¸å¿ƒæ•°æ®ç»“æ„**: Pandas DataFrameï¼Œè®°å½•æ‰€æœ‰å¯å‰ªæå•å…ƒ

```python
# core/methods/global_pruning.py
def build_global_group_table(model, importance_method='taylor'):
    """
    æ„å»ºå…¨å±€Groupåˆ†æè¡¨

    è¿”å›DataFrame:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ layer_idx â”‚ group_type â”‚ group_idx â”‚ importance â”‚   cost   â”‚   score    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚     5     â”‚ attention  â”‚     2     â”‚  0.123456  â”‚ 6291456  â”‚ 1.962e-08  â”‚ â† æœ€ä½score
    â”‚    12     â”‚ mlp        â”‚   1024    â”‚  0.234567  â”‚  12288   â”‚ 1.909e-05  â”‚
    â”‚    ...    â”‚   ...      â”‚   ...     â”‚    ...     â”‚   ...    â”‚    ...     â”‚
    â”‚    15     â”‚ attention  â”‚     4     â”‚  9.876543  â”‚ 6291456  â”‚ 1.884e-05  â”‚ â† æœ€é«˜score
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    æŒ‰scoreå‡åºæ’åˆ—ï¼Œscoreæœ€ä½çš„ä¼˜å…ˆå‰ªæ
    """
    table = []

    for layer_idx in range(num_layers):
        layer = model.model.layers[layer_idx]

        # ========== Attention Groups ==========
        # è®¡ç®—æ¯ä¸ªGQA groupçš„é‡è¦æ€§
        attn_importance = compute_attention_importance(layer, method)

        for kv_idx in range(num_kv_heads):
            group_info = {
                'layer_idx': layer_idx,
                'group_type': 'attention',
                'group_idx': kv_idx,
                'importance': attn_importance[kv_idx],
                'cost': compute_gqa_group_cost(head_dim, gqa_ratio, hidden_dim),
                # cost = 1 KV head + 4 Q heads çš„å‚æ•°é‡
            }
            group_info['score'] = group_info['importance'] / group_info['cost']
            table.append(group_info)

        # ========== MLP Channels ==========
        mlp_importance = compute_mlp_importance(layer, method)

        for channel_idx in range(intermediate_size):
            channel_info = {
                'layer_idx': layer_idx,
                'group_type': 'mlp',
                'group_idx': channel_idx,
                'importance': mlp_importance[channel_idx],
                'cost': compute_mlp_channel_cost(hidden_dim),
                # cost = gate_proj + up_proj + down_proj çš„å‚æ•°é‡
            }
            channel_info['score'] = channel_info['importance'] / channel_info['cost']
            table.append(channel_info)

    # æŒ‰scoreæ’åº
    df = pd.DataFrame(table)
    df = df.sort_values('score').reset_index(drop=True)

    return df
```

**å‚æ•°æˆæœ¬è®¡ç®—**:

```python
# Attention Group (1 KV + 4 Q heads)
def compute_gqa_group_cost(head_dim=128, gqa_ratio=4, hidden_dim=4096):
    """
    æˆæœ¬ = q_proj + k_proj + v_proj + o_proj çš„å‚æ•°é‡

    q_proj: hidden_dim Ã— (gqa_ratio Ã— head_dim)  [4ä¸ªQ heads]
    k_proj: hidden_dim Ã— head_dim                [1ä¸ªKV head]
    v_proj: hidden_dim Ã— head_dim                [1ä¸ªKV head]
    o_proj: (gqa_ratio Ã— head_dim) Ã— hidden_dim  [4ä¸ªQ headsçš„è¾“å‡º]
    """
    q_params = hidden_dim * (gqa_ratio * head_dim)  # 4096 Ã— 512 = 2,097,152
    k_params = hidden_dim * head_dim                # 4096 Ã— 128 =   524,288
    v_params = hidden_dim * head_dim                # 4096 Ã— 128 =   524,288
    o_params = (gqa_ratio * head_dim) * hidden_dim  # 512 Ã— 4096 = 2,097,152

    total = q_params + k_params + v_params + o_params
    # å¯¹äºLLaMA-3-8B: 6,291,456 å‚æ•°/ç»„

    return total

# MLP Channel (å•ä¸ªç¥ç»å…ƒ)
def compute_mlp_channel_cost(hidden_dim=4096):
    """
    æˆæœ¬ = gate_projçš„ä¸€è¡Œ + up_projçš„ä¸€è¡Œ + down_projçš„ä¸€åˆ—
    """
    gate_params = hidden_dim  # 4096
    up_params = hidden_dim    # 4096
    down_params = hidden_dim  # 4096

    total = gate_params + up_params + down_params
    # å¯¹äºLLaMA-3-8B: 12,288 å‚æ•°/é€šé“

    return total
```

**é‡è¦æ€§è®¡ç®—æ–¹æ³•** (æ”¯æŒä¸‰ç§):

**æ–¹æ³•1: ä¸€é˜¶Taylor (`--importance_method taylor`)**
```python
def compute_taylor_importance(weight, gradient):
    """
    ä¸€é˜¶æ³°å‹’å±•å¼€

    Î”L â‰ˆ Î£ (âˆ‚L/âˆ‚Î¸) Â· Î”Î¸

    å¦‚æœå‰ªæ‰æŸä¸ªå‚æ•°ï¼ˆÎ”Î¸ = -Î¸ï¼‰ï¼Œåˆ™:
    Î”L â‰ˆ -Î¸ Â· (âˆ‚L/âˆ‚Î¸)

    é‡è¦æ€§ = |Î”L| = |Î¸ Â· g|
    """
    return (weight * gradient).abs().sum()
```

**æ–¹æ³•2: äºŒé˜¶Taylor (`--importance_method taylor_2nd`)**
```python
def compute_taylor_2nd_importance(weight, gradient, hessian_diag):
    """
    äºŒé˜¶æ³°å‹’å±•å¼€

    Î”L â‰ˆ gÂ·Î”Î¸ + 0.5Â·Î”Î¸^TÂ·HÂ·Î”Î¸

    è¿‘ä¼¼Hessianå¯¹è§’çº¿: H_diag â‰ˆ E[gÂ²]

    é‡è¦æ€§ = |Î¸Â·g| + 0.5Â·|Î¸Â²Â·H_diag|
    """
    first_order = (weight * gradient).abs()
    second_order = 0.5 * (weight ** 2 * hessian_diag).abs()

    return (first_order + second_order).sum()
```

**Hessianå¯¹è§’çº¿è¿‘ä¼¼**:
```python
# ç´¯åŠ å¤šä¸ªbatchçš„æ¢¯åº¦å¹³æ–¹
hessian_diag = {}
for name, param in model.named_parameters():
    hessian_diag[name] = torch.zeros_like(param, device='cpu')

for batch in batches:
    loss = model(batch).loss
    loss.backward()

    for name, param in model.named_parameters():
        # H_diag â‰ˆ (1/N) Î£ gÂ²
        hessian_diag[name] += (param.grad ** 2).cpu() / num_batches
```

**å†…å­˜ä¼˜åŒ–**: Hessianå­˜å‚¨åœ¨CPUä¸Šï¼Œä½¿ç”¨æ—¶å†ç§»åˆ°GPU
```python
# åˆå§‹åŒ–åœ¨CPU
hessian_diag[name] = torch.zeros_like(param, device='cpu')

# ç´¯åŠ æ—¶ä¹Ÿåœ¨CPU
hessian_diag[name] += (param.grad ** 2).cpu() / num_batches

# ä½¿ç”¨æ—¶ç§»åˆ°GPU
hess = hessian_diag[full_name].to(weight.device)
second_order = 0.5 * (weight ** 2 * hess).abs()
```

**èŠ‚çœæ˜¾å­˜**: ~16GB (å¯¹äºLLaMA-3-8B)

**æ–¹æ³•3: Wanda (`--importance_method wanda`)**
```python
def compute_wanda_importance(weight, activation):
    """
    Wanda: Weight and Activation

    é‡è¦æ€§ = |Î¸ Â· A|

    å…¶ä¸­Aæ˜¯å¹³å‡æ¿€æ´»å€¼
    """
    return (weight * activation).abs().sum()
```

**æ¿€æ´»å€¼æ”¶é›†**:
```python
activations = {}

def hook(module, input, output):
    # è®°å½•è¾“å…¥æ¿€æ´»çš„ç»Ÿè®¡é‡
    act = input[0].detach().abs().mean(dim=(0, 1))  # å¹³å‡åˆ°ç‰¹å¾ç»´åº¦
    activations[module_name] = act.cpu()

# æ³¨å†Œhooks
for name, module in model.named_modules():
    if isinstance(module, nn.Linear):
        module.register_forward_hook(hook)

# å‰å‘ä¼ æ’­
model(input_ids)
```

#### 1.3.3 å…¨å±€å‰ªææ‰§è¡Œ

```python
def select_groups_to_prune(df, pruning_ratio, total_params):
    """
    ä»å…¨å±€åˆ†æè¡¨ä¸­é€‰æ‹©è¦å‰ªæçš„groups

    è´ªå¿ƒç­–ç•¥: æŒ‰scoreä»ä½åˆ°é«˜ç´¯åŠ ï¼Œç›´åˆ°è¾¾åˆ°é¢„ç®—
    """
    target_prune_params = total_params * pruning_ratio

    cumsum = df['cost'].cumsum()

    # æ‰¾åˆ°ç´¯åŠ å’Œåˆšå¥½è¶…è¿‡ç›®æ ‡çš„ä½ç½®
    cutoff_idx = (cumsum <= target_prune_params).sum()

    groups_to_prune = df.iloc[:cutoff_idx]

    return groups_to_prune

def apply_global_pruning(model, groups_to_prune_df):
    """
    æ‰§è¡Œå…¨å±€å‰ªæ
    """
    # æŒ‰å±‚ç»„ç»‡å‰ªæä¿¡æ¯
    for layer_idx in range(num_layers):
        layer_data = groups_to_prune_df[
            groups_to_prune_df['layer_idx'] == layer_idx
        ]

        # Attentionå‰ªæ
        attn_groups = layer_data[
            layer_data['group_type'] == 'attention'
        ]['group_idx'].tolist()

        if len(attn_groups) > 0:
            keep_indices = [i for i in range(num_kv_heads)
                           if i not in attn_groups]
            prune_attention_by_gqa_groups(layer, keep_indices)

        # MLPå‰ªæ
        mlp_channels = layer_data[
            layer_data['group_type'] == 'mlp'
        ]['group_idx'].tolist()

        if len(mlp_channels) > 0:
            keep_indices = [i for i in range(intermediate_size)
                           if i not in mlp_channels]
            prune_mlp_channels(layer, keep_indices)
```

#### 1.3.4 è‡ªåŠ¨æ·±åº¦å‰ªæ

**ç°è±¡**: å…¨å±€å‰ªæå¯èƒ½å¯¼è‡´æŸäº›å±‚è¢«**å®Œå…¨å‰ªç©º**

```
Layer 12:
  Attention: 8 KV heads â†’ 0 KV heads (å…¨éƒ¨scoreéƒ½å¾ˆä½)
  MLP: 14336 channels â†’ 0 channels (å…¨éƒ¨scoreéƒ½å¾ˆä½)

ç»“æœ: Layer 12 å®Œå…¨æ²¡æœ‰å‚æ•°äº†ï¼
```

**åŸå› **: è¯¥å±‚çš„æ‰€æœ‰ç»„ä»¶çš„scoreéƒ½ä½äºå…¶ä»–å±‚çš„å¹³å‡æ°´å¹³ï¼ˆç¬¦åˆUå‹åˆ†å¸ƒï¼‰

**è§£å†³æ–¹æ¡ˆ**: `--remove_empty_layers`

```python
def remove_empty_layers(model, empty_layers):
    """
    ç§»é™¤è¢«å‰ªç©ºçš„å±‚ï¼Œå®ç°è‡ªåŠ¨æ·±åº¦å‰ªæ

    è¿™æ˜¯width pruning â†’ depth pruningçš„è‡ªç„¶è¿‡æ¸¡
    """
    keep_layers = [i for i in range(num_layers)
                   if i not in empty_layers]

    new_layers = nn.ModuleList([model.model.layers[i]
                                for i in keep_layers])

    model.model.layers = new_layers
    model.config.num_hidden_layers = len(keep_layers)
```

**æ•ˆæœ**:
```
åŸå§‹: 32å±‚ï¼Œæ¯å±‚4096ç»´
å‰ªæåï¼ˆ25%å‚æ•°ï¼‰: 28å±‚ï¼ˆè‡ªåŠ¨ç§»é™¤äº†4å±‚ï¼‰ï¼Œæ¯å±‚ç»´åº¦ä¸ç­‰

æ·±åº¦å‰ªæ + å®½åº¦å‰ªæ çš„æ··åˆç­–ç•¥ï¼
```

---

### 1.4 ä¸‰ä¸ªé˜¶æ®µçš„å¯¹æ¯”

| ç‰¹æ€§ | v0.1 åŸºç¡€å‰ªæ | v1.0 å±‚çº§+æœç´¢ | v2.0 å…¨å±€å‰ªæ |
|------|--------------|----------------|---------------|
| **å‰ªæç²’åº¦** | é€å±‚ç‹¬ç«‹ | å±‚çº§éå‡è¡¡ | å…¨å±€æœ€ä¼˜ |
| **Attention:MLP** | å›ºå®šæ¯”ä¾‹ | å¯é…ç½®+è‡ªåŠ¨æœç´¢ | è‡ªåŠ¨å¹³è¡¡ |
| **å±‚é—´å¯¹æ¯”** | âŒ ä¸æ”¯æŒ | âœ… å±‚é‡è¦æ€§è¯„ä¼° | âœ… è·¨å±‚å…¨å±€å¯¹æ¯” |
| **ç»„ä»¶å¯¹æ¯”** | âŒ ä¸æ”¯æŒ | âŒ åˆ†å¼€å¤„ç† | âœ… Attn vs MLPç»Ÿä¸€å¯¹æ¯” |
| **æ·±åº¦å‰ªæ** | âŒ ä¸æ”¯æŒ | âŒ æ‰‹åŠ¨å±‚é€‰æ‹© | âœ… è‡ªåŠ¨ç§»é™¤ç©ºå±‚ |
| **å‚æ•°æœç´¢** | æ—  | è‡ªåŠ¨åˆ†å¸ƒæœç´¢ | æ— éœ€ï¼ˆè‡ªåŠ¨å¹³è¡¡ï¼‰ |
| **ç†è®ºåŸºç¡€** | Tayloré‡è¦æ€§ | å±‚çº§+Taylor | åˆ†æ•°èƒŒåŒ…é—®é¢˜ |
| **ä¼˜åŒ–ç›®æ ‡** | å±€éƒ¨æœ€ä¼˜ | å±‚çº§æœ€ä¼˜ | å…¨å±€æœ€ä¼˜ |
| **è®¡ç®—å¤æ‚åº¦** | O(LÃ—N) | O(LÃ—N + search) | O(LÃ—NÃ—log(LÃ—N)) |
| **é€‚ç”¨åœºæ™¯** | å¿«é€ŸåŸå‹ | ç”Ÿäº§ç¯å¢ƒ | æè‡´æ€§èƒ½ |

**L**: å±‚æ•°ï¼Œ**N**: æ¯å±‚ç¥ç»å…ƒæ•°

---

## 2. æ ¸å¿ƒæŠ€æœ¯æ¡†æ¶

### 2.1 GQAæ¶æ„æ„ŸçŸ¥

**GQA (Grouped Query Attention)**: LLaMA-3çš„æ ¸å¿ƒæ¶æ„

```
ä¼ ç»ŸMulti-Head Attention:
  Q heads: 32ä¸ªï¼Œæ¯ä¸ª128ç»´
  K heads: 32ä¸ªï¼Œæ¯ä¸ª128ç»´  â† æ¯ä¸ªQæœ‰ç‹¬ç«‹çš„KV
  V heads: 32ä¸ªï¼Œæ¯ä¸ª128ç»´

GQA (4:1æ¯”ä¾‹):
  Q heads: 32ä¸ªï¼Œæ¯ä¸ª128ç»´
  K heads: 8ä¸ªï¼Œæ¯ä¸ª128ç»´   â† 4ä¸ªQå…±äº«1ä¸ªKV
  V heads: 8ä¸ªï¼Œæ¯ä¸ª128ç»´

ä¼˜åŠ¿: KV cacheå‡å°‘75%ï¼Œæ¨ç†åŠ é€Ÿ

çº¦æŸ: å‰ªææ—¶å¿…é¡»ä¿æŒ4:1æ¯”ä¾‹ï¼
```

**å‰ªæå•ä½**: GQAç»„ï¼ˆ1 KV + 4 Qï¼‰

```python
# âŒ é”™è¯¯: å•ç‹¬å‰ªQæˆ–KV
prune_q_heads([0, 5, 10])  # ç ´å4:1æ¯”ä¾‹
prune_kv_heads([2])        # Qæ‰¾ä¸åˆ°å¯¹åº”çš„KV

# âœ… æ­£ç¡®: æŒ‰ç»„å‰ªæ
prune_gqa_group(kv_idx=2)  # åŒæ—¶å‰ªæ‰KV #2 å’Œå¯¹åº”çš„4ä¸ªQ
# å‰ªæå‰: 32Q:8KV (Q8-11å¯¹åº”KV2)
# å‰ªæå: 28Q:7KV (Q8-11å’ŒKV2éƒ½è¢«ç§»é™¤)
```

**æƒé‡çŸ©é˜µåˆ‡ç‰‡**:
```python
def prune_attention_by_gqa_groups(layer, keep_kv_indices, head_dim, gqa_ratio):
    """
    æŒ‰GQAç»„å‰ªæattention

    å‡è®¾keep_kv_indices = [0, 1, 3, 5, 6, 7] (ä¿ç•™6ä¸ªKV)
    åˆ™å¯¹åº”ä¿ç•™Q indices = [0-3, 4-7, 12-15, 20-23, 24-27, 28-31]
    """
    num_kv_heads = len(keep_kv_indices)

    # è®¡ç®—å¯¹åº”çš„Q indices
    keep_q_indices = []
    for kv_idx in keep_kv_indices:
        q_start = kv_idx * gqa_ratio
        q_end = q_start + gqa_ratio
        keep_q_indices.extend(range(q_start, q_end))

    # åˆ‡ç‰‡æƒé‡çŸ©é˜µ
    # q_proj: [hidden_dim, num_q_heads * head_dim]
    q_dim_indices = torch.cat([
        torch.arange(q_idx * head_dim, (q_idx + 1) * head_dim)
        for q_idx in keep_q_indices
    ])
    layer.self_attn.q_proj.weight = nn.Parameter(
        layer.self_attn.q_proj.weight[:, q_dim_indices]
    )

    # k_proj, v_proj: [hidden_dim, num_kv_heads * head_dim]
    kv_dim_indices = torch.cat([
        torch.arange(kv_idx * head_dim, (kv_idx + 1) * head_dim)
        for kv_idx in keep_kv_indices
    ])
    layer.self_attn.k_proj.weight = nn.Parameter(
        layer.self_attn.k_proj.weight[:, kv_dim_indices]
    )
    layer.self_attn.v_proj.weight = nn.Parameter(
        layer.self_attn.v_proj.weight[:, kv_dim_indices]
    )

    # o_proj: [num_q_heads * head_dim, hidden_dim]
    layer.self_attn.o_proj.weight = nn.Parameter(
        layer.self_attn.o_proj.weight[q_dim_indices, :]
    )

    # æ›´æ–°é…ç½®
    layer.self_attn.num_heads = num_kv_heads * gqa_ratio  # æ–°Qæ•°é‡
    layer.self_attn.num_key_value_heads = num_kv_heads    # æ–°KVæ•°é‡

    return num_kv_heads * gqa_ratio, num_kv_heads  # (num_q, num_kv)
```

### 2.2 Tayloré‡è¦æ€§ç†è®º

**ä¸€é˜¶æ³°å‹’å±•å¼€**:

```
å‡è®¾å‰ªæ‰å‚æ•°Î¸ï¼ŒæŸå¤±å‡½æ•°çš„å˜åŒ–é‡:

Î”L = L(Î¸=0) - L(Î¸) â‰ˆ -(âˆ‚L/âˆ‚Î¸)Â·Î¸

é‡è¦æ€§å®šä¹‰:
  I = |Î”L| = |Î¸ Â· g|

å…¶ä¸­ g = âˆ‚L/âˆ‚Î¸ (æ¢¯åº¦)

ç›´è§‰:
  - Î¸å¤§ä¸”gå¤§ â†’ å‰ªæ‰åæŸå¤±å¤§ â†’ é‡è¦
  - Î¸å°æˆ–gå° â†’ å‰ªæ‰åæŸå¤±å° â†’ ä¸é‡è¦
```

**äºŒé˜¶æ³°å‹’å±•å¼€**:

```
æ›´ç²¾ç¡®çš„è¿‘ä¼¼:

Î”L â‰ˆ -(âˆ‚L/âˆ‚Î¸)Â·Î¸ - 0.5Â·Î¸^TÂ·HÂ·Î¸

å…¶ä¸­Hæ˜¯HessiançŸ©é˜µ (âˆ‚Â²L/âˆ‚Î¸Â²)

å¯¹è§’è¿‘ä¼¼: H â‰ˆ diag(E[gÂ²])

é‡è¦æ€§:
  I = |Î¸Â·g| + 0.5Â·|Î¸Â²Â·H_diag|

  ç¬¬ä¸€é¡¹: ä¸€é˜¶è´¡çŒ®ï¼ˆæ¢¯åº¦æ–¹å‘ï¼‰
  ç¬¬äºŒé¡¹: äºŒé˜¶è´¡çŒ®ï¼ˆæ›²ç‡ä¿¡æ¯ï¼‰
```

**ä¸ºä»€ä¹ˆäºŒé˜¶æ›´å¥½ï¼Ÿ**

```
è€ƒè™‘ä¸¤ä¸ªå‚æ•°:

å‚æ•°A: Î¸=0.5, g=2.0, H=0.1
  ä¸€é˜¶: |0.5 Ã— 2.0| = 1.0
  äºŒé˜¶: |0.5 Ã— 2.0| + 0.5Â·|0.5Â² Ã— 0.1| = 1.0125

å‚æ•°B: Î¸=0.5, g=2.0, H=10.0 (é«˜æ›²ç‡)
  ä¸€é˜¶: |0.5 Ã— 2.0| = 1.0
  äºŒé˜¶: |0.5 Ã— 2.0| + 0.5Â·|0.5Â² Ã— 10.0| = 2.25

ä¸€é˜¶æ— æ³•åŒºåˆ†ï¼Œä½†äºŒé˜¶èƒ½è¯†åˆ«å‡ºBå¤„äºæŸå¤±å‡½æ•°çš„é™¡å³­åŒºåŸŸï¼Œæ›´é‡è¦ï¼
```

### 2.3 MLPå‰ªææ–¹æ³•

**MLPç»“æ„**:
```
LLaMA MLPä½¿ç”¨SwiGLUæ¿€æ´»:

x â†’ gate_proj â†’ SiLU â”€â”€â”
                       Ã— â†’ down_proj â†’ out
x â†’ up_proj   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

gate_proj: [hidden_dim, intermediate_size]  (4096 â†’ 14336)
up_proj:   [hidden_dim, intermediate_size]  (4096 â†’ 14336)
down_proj: [intermediate_size, hidden_dim]  (14336 â†’ 4096)
```

**å‰ªæç­–ç•¥**: æŒ‰é€šé“å‰ªæ

```python
def compute_mlp_channel_importance(layer, method='taylor'):
    """
    è®¡ç®—æ¯ä¸ªMLPé€šé“çš„é‡è¦æ€§

    ä¸€ä¸ªé€šé“ = gate_projçš„ä¸€è¡Œ + up_projçš„ä¸€è¡Œ + down_projçš„ä¸€åˆ—
    """
    intermediate_size = layer.mlp.gate_proj.out_features
    channel_importance = torch.zeros(intermediate_size)

    if method == 'taylor':
        # èšåˆä¸‰ä¸ªæŠ•å½±çš„é‡è¦æ€§
        for channel_idx in range(intermediate_size):
            # gate_projçš„ç¬¬channel_idxè¡Œ
            gate_imp = (layer.mlp.gate_proj.weight[channel_idx, :] *
                       layer.mlp.gate_proj.weight.grad[channel_idx, :]).abs().sum()

            # up_projçš„ç¬¬channel_idxè¡Œ
            up_imp = (layer.mlp.up_proj.weight[channel_idx, :] *
                     layer.mlp.up_proj.weight.grad[channel_idx, :]).abs().sum()

            # down_projçš„ç¬¬channel_idxåˆ—
            down_imp = (layer.mlp.down_proj.weight[:, channel_idx] *
                       layer.mlp.down_proj.weight.grad[:, channel_idx]).abs().sum()

            # æ€»é‡è¦æ€§ = ä¸‰ä¸ªæŠ•å½±çš„åŠ æƒå¹³å‡
            channel_importance[channel_idx] = gate_imp + up_imp + down_imp

    return channel_importance

def prune_mlp_channels(layer, keep_indices):
    """
    å‰ªæMLPé€šé“
    """
    keep_indices_tensor = torch.tensor(keep_indices, device=layer.mlp.gate_proj.weight.device)

    # gate_projå’Œup_proj: ä¿ç•™æŒ‡å®šçš„è¡Œ
    layer.mlp.gate_proj.weight = nn.Parameter(
        layer.mlp.gate_proj.weight[keep_indices_tensor, :]
    )
    layer.mlp.up_proj.weight = nn.Parameter(
        layer.mlp.up_proj.weight[keep_indices_tensor, :]
    )

    # down_proj: ä¿ç•™æŒ‡å®šçš„åˆ—
    layer.mlp.down_proj.weight = nn.Parameter(
        layer.mlp.down_proj.weight[:, keep_indices_tensor]
    )

    # æ›´æ–°ç»´åº¦
    new_size = len(keep_indices)
    layer.mlp.gate_proj.out_features = new_size
    layer.mlp.up_proj.out_features = new_size
    layer.mlp.down_proj.in_features = new_size
```

### 2.4 å¾®è°ƒæ¢å¤æœºåˆ¶

**å…¨å‚æ•°å¾®è°ƒ vs LoRA**:

```
å…¨å‚æ•°å¾®è°ƒ (Full Fine-tuning):
  - æ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°
  - æ•ˆæœå¥½ï¼Œä½†æ˜¾å­˜éœ€æ±‚é«˜
  - é€‚åˆè½»åº¦å‰ªæï¼ˆ<20%ï¼‰

LoRAå¾®è°ƒ (Low-Rank Adaptation):
  - å†»ç»“åŸå§‹æƒé‡ï¼Œåªè®­ç»ƒä½ç§©å¢é‡
  - æ˜¾å­˜å‹å¥½ï¼Œé€Ÿåº¦å¿«
  - é€‚åˆä¸­é‡åº¦å‰ªæï¼ˆ20-40%ï¼‰
```

**LoRAåŸç†**:

```
åŸå§‹æƒé‡: W âˆˆ R^(dÃ—k)

å†»ç»“Wï¼Œæ·»åŠ ä½ç§©åˆ†è§£:
  W' = W + Î”W
  Î”W = BÂ·A

å…¶ä¸­:
  A âˆˆ R^(rÃ—k), B âˆˆ R^(dÃ—r)
  r << min(d, k)  (é€šå¸¸r=8-16)

å¯è®­ç»ƒå‚æ•°:
  å…¨å‚æ•°: dÃ—k
  LoRA: dÃ—r + rÃ—k = rÃ—(d+k) << dÃ—k

ç¤ºä¾‹ (LLaMA-3, r=8):
  q_proj: 4096Ã—4096 = 16M å‚æ•°
  LoRA:   4096Ã—8 + 8Ã—4096 = 65K å‚æ•° (ä»…0.4%)
```

**å®ç°**:

```python
# core/trainer/finetuner.py
class FineTuner:
    def finetune(self, method='lora', lora_r=8, lora_alpha=16, ...):
        """
        å‰ªæåå¾®è°ƒ

        Args:
            method: 'full' æˆ– 'lora'
            lora_r: LoRAç§©
            lora_alpha: ç¼©æ”¾ç³»æ•° (é€šå¸¸=2Ã—r)
        """
        if method == 'lora':
            # é…ç½®LoRA
            lora_config = LoraConfig(
                r=lora_r,
                lora_alpha=lora_alpha,
                target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj',
                               'gate_proj', 'up_proj', 'down_proj'],
                lora_dropout=0.05,
                bias='none',
                task_type='CAUSAL_LM'
            )

            # åº”ç”¨LoRA
            model = get_peft_model(model, lora_config)

            # åªæœ‰LoRAå‚æ•°å¯è®­ç»ƒ
            trainable_params = sum(p.numel() for p in model.parameters()
                                  if p.requires_grad)
            # é€šå¸¸ < 1% çš„åŸå§‹å‚æ•°é‡

        # è®­ç»ƒå¾ªç¯
        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                         lr=lr, weight_decay=weight_decay)

        for epoch in range(epochs):
            for batch in dataloader:
                loss = model(**batch).loss
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

                optimizer.step()
                optimizer.zero_grad()

        if method == 'lora':
            # åˆå¹¶LoRAæƒé‡å›åŸºç¡€æ¨¡å‹
            model = model.merge_and_unload()
```

---

## 3. å½“å‰æ¶æ„åˆ†æ

### 3.1 ä»£ç åº“ç»“æ„

```
GAQ-Aware-Prune/
â”œâ”€â”€ ğŸ“œ ä¸»è„šæœ¬ (Entry Points)
â”‚   â”œâ”€â”€ llama3_unbalanced_pruning_gqa_aware.py  â­ å±‚çº§å‰ªæï¼ˆç”Ÿäº§ä¸»åŠ›ï¼‰
â”‚   â”œâ”€â”€ llama3_global_pruning.py                 â­ å…¨å±€å‰ªæï¼ˆå®éªŒæ€§ï¼‰
â”‚   â”œâ”€â”€ search_optimal_distribution.py           ğŸ” è‡ªåŠ¨è¶…å‚æœç´¢
â”‚   â”œâ”€â”€ demo_global_pruning.py                   ğŸ§ª å…¨å±€å‰ªædemo
â”‚   â”œâ”€â”€ test_finetuning.py                       ğŸ§ª å¾®è°ƒæµ‹è¯•
â”‚   â”œâ”€â”€ evaluate_models.py                       ğŸ“Š æ¨¡å‹å¯¹æ¯”è¯„ä¼°
â”‚   â””â”€â”€ diagnose_model.py                        ğŸ”§ æ¨¡å‹å¥åº·æ£€æŸ¥
â”‚
â”œâ”€â”€ ğŸ§© æ ¸å¿ƒåº“ (core/)
â”‚   â”œâ”€â”€ methods/                                 # å‰ªæç®—æ³•
â”‚   â”‚   â”œâ”€â”€ gqa_aware.py                         # GQAæ„ŸçŸ¥å‰ªæ
â”‚   â”‚   â””â”€â”€ global_pruning.py                    # å…¨å±€åˆ†æ•°èƒŒåŒ…å‰ªæ
â”‚   â”‚
â”‚   â”œâ”€â”€ importance/                              # é‡è¦æ€§åˆ†æ
â”‚   â”‚   â””â”€â”€ layer_analyzer.py                    # å±‚çº§é‡è¦æ€§+å‰ªæç‡åˆ†é…
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/                                # æ•°æ®åŠ è½½
â”‚   â”‚   â””â”€â”€ example_samples.py                   # WikiText2, C4
â”‚   â”‚
â”‚   â”œâ”€â”€ trainer/                                 # å¾®è°ƒ
â”‚   â”‚   â””â”€â”€ finetuner.py                         # Full + LoRAå¾®è°ƒ
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluator/                               # è¯„ä¼° (å·²åºŸå¼ƒ)
â”‚   â”‚   â””â”€â”€ ppl.py                               # â†’ è¿ç§»åˆ°evaluation/
â”‚   â”‚
â”‚   â””â”€â”€ utils/                                   # å·¥å…·
â”‚       â”œâ”€â”€ logger.py                            # æ—¥å¿—ç³»ç»Ÿ
â”‚       â””â”€â”€ get_best_gpu.py                      # GPUé€‰æ‹©
â”‚
â”œâ”€â”€ ğŸ“Š è¯„ä¼°å¥—ä»¶ (evaluation/)
â”‚   â”œâ”€â”€ run_evaluation.py                        # ç»Ÿä¸€è¯„ä¼°å…¥å£
â”‚   â”œâ”€â”€ convert_checkpoint_to_hf.py              # æ£€æŸ¥ç‚¹è½¬æ¢
â”‚   â”œâ”€â”€ clean_dataset_cache.py                   # ç¼“å­˜æ¸…ç†
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ performance.py                       # PPL, Zero-shot, Few-shot
â”‚   â”‚   â””â”€â”€ efficiency.py                        # ååé‡, å†…å­˜
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ model_loader.py                      # æ¨¡å‹åŠ è½½
â”‚
â”œâ”€â”€ ğŸ“– æ–‡æ¡£ (Documentation)
â”‚   â”œâ”€â”€ README.md                                # é¡¹ç›®æ¦‚è§ˆ
â”‚   â”œâ”€â”€ CLAUDE.md                                # AIåŠ©æ‰‹å¼€å‘æŒ‡å—
â”‚   â”œâ”€â”€ GLOBAL_PRUNING_GUIDE.md                  # å…¨å±€å‰ªæä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ PARAMETERS_GUIDE.md                      # å‚æ•°é€‰æ‹©æŒ‡å—
â”‚   â”œâ”€â”€ SEARCH_EXAMPLE.md                        # æœç´¢è„šæœ¬ç¤ºä¾‹
â”‚   â”œâ”€â”€ DATASET_SELECTION_GUIDE.md               # æ•°æ®é›†é€‰æ‹©
â”‚   â””â”€â”€ IMPLEMENTATION_SUMMARY.md                # å®ç°æ€»ç»“
â”‚
â””â”€â”€ ğŸ“‚ è¾“å‡º (prune_log/, gitignored)
    â””â”€â”€ {experiment_name}/
        â”œâ”€â”€ description.txt                      # å®éªŒé…ç½®
        â”œâ”€â”€ layer_importance_config.json         # å±‚é‡è¦æ€§
        â”œâ”€â”€ pruning_strategy.png                 # å¯è§†åŒ–
        â”œâ”€â”€ pytorch_model.bin                    # å‰ªææ¨¡å‹
        â”œâ”€â”€ pytorch_model_finetuned.bin          # å¾®è°ƒæ¨¡å‹
        â””â”€â”€ {timestamp}/
            â””â”€â”€ training.log                     # è¯¦ç»†æ—¥å¿—
```

### 3.2 æ¨¡å—ä¾èµ–å…³ç³»

```
ä¸»è„šæœ¬å±‚:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llama3_unbalanced_pruning_gqa_aware.py                  â”‚
â”‚ llama3_global_pruning.py                                â”‚
â”‚ search_optimal_distribution.py                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                     â”‚                     â”‚
         â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ core.methods    â”‚  â”‚ core.importance  â”‚  â”‚ core.trainer    â”‚
â”‚ â”œâ”€ gqa_aware    â”‚  â”‚ â””â”€ layer_analyzerâ”‚  â”‚ â””â”€ finetuner    â”‚
â”‚ â””â”€ global_prune â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ core.datasets   core.evaluator   core.utils             â”‚
â”‚ â””â”€ examples     â””â”€ ppl           â”œâ”€ logger              â”‚
â”‚                                   â””â”€ get_best_gpu       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ External: transformers, torch, datasets, pandas         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 æ•°æ®æµåˆ†æ

**å±‚çº§å‰ªææµç¨‹** (`llama3_unbalanced_pruning_gqa_aware.py`):

```
1. åŠ è½½æ¨¡å‹
   â†“
2. å±‚é‡è¦æ€§åˆ†æ
   â”œâ”€ LayerImportanceAnalyzer.measure_layer_importance_by_removal()
   â””â”€ è¾“å‡º: {layer_idx: importance_score}
   â†“
3. è®¡ç®—æ¯å±‚å‰ªæç‡
   â”œâ”€ UnbalancedStructuredPruningCalculator.compute_layer_pruning_rates()
   â”œâ”€ è¾“å…¥: å±‚é‡è¦æ€§, ç›®æ ‡æ€»å‰ªæç‡, distribution
   â””â”€ è¾“å‡º: {layer_idx: {'attn_rate': x, 'mlp_rate': y}}
   â†“
4. é€å±‚å‰ªæ
   â”œâ”€ å¯¹æ¯å±‚:
   â”‚  â”œâ”€ è®¡ç®—æ¢¯åº¦ (get_examples â†’ forward â†’ backward)
   â”‚  â”œâ”€ Attention: compute_gqa_group_importance â†’ select â†’ prune
   â”‚  â””â”€ MLP: compute_mlp_channel_importance â†’ select â†’ prune
   â””â”€ è¾“å‡º: å‰ªæåçš„æ¨¡å‹
   â†“
5. å¾®è°ƒï¼ˆå¯é€‰ï¼‰
   â”œâ”€ FineTuner.finetune(method='lora')
   â””â”€ è¾“å‡º: å¾®è°ƒåçš„æ¨¡å‹
   â†“
6. è¯„ä¼°
   â””â”€ PPLMetric(model) â†’ è¾“å‡ºPPL
```

**å…¨å±€å‰ªææµç¨‹** (`llama3_global_pruning.py`):

```
1. åŠ è½½æ¨¡å‹
   â†“
2. è®¡ç®—æ¢¯åº¦/æ¿€æ´»
   â”œâ”€ Taylor: forward â†’ backward (ç´¯åŠ æ¢¯åº¦)
   â”œâ”€ Taylor_2nd: forward â†’ backward (ç´¯åŠ æ¢¯åº¦å¹³æ–¹)
   â””â”€ Wanda: forward (æ”¶é›†æ¿€æ´»)
   â†“
3. æ„å»ºå…¨å±€åˆ†æè¡¨
   â”œâ”€ build_global_group_table()
   â”œâ”€ å¯¹æ¯å±‚æ¯ä¸ªgroup:
   â”‚  â”œâ”€ importance = compute_importance(method)
   â”‚  â”œâ”€ cost = compute_cost()
   â”‚  â””â”€ score = importance / cost
   â””â”€ è¾“å‡º: DataFrame (æŒ‰scoreæ’åº)
   â†“
4. é€‰æ‹©å‰ªægroups
   â”œâ”€ select_groups_to_prune(df, pruning_ratio)
   â””â”€ è´ªå¿ƒç´¯åŠ : ä»scoreæœ€ä½å¼€å§‹ï¼Œç›´åˆ°è¾¾åˆ°å‚æ•°é¢„ç®—
   â†“
5. æ‰§è¡Œå…¨å±€å‰ªæ
   â”œâ”€ apply_global_pruning(model, groups_to_prune)
   â””â”€ æŒ‰å±‚åº”ç”¨å‰ªæå†³ç­–
   â†“
6. ç§»é™¤ç©ºå±‚ï¼ˆå¯é€‰ï¼‰
   â”œâ”€ remove_empty_layers(model, empty_layers)
   â””â”€ æ·±åº¦å‰ªæ
   â†“
7. å¾®è°ƒ & è¯„ä¼°
   â””â”€ åŒå±‚çº§å‰ªæ
```

### 3.4 å¯ç²¾ç®€çš„åœ°æ–¹

#### 3.4.1 ä»£ç å†—ä½™

**é—®é¢˜1: PPLè¯„ä¼°æ¨¡å—é‡å¤**

```
å½“å‰:
  core/evaluator/ppl.py        â† æ—§ç‰ˆæœ¬
  evaluation/metrics/ppl.py    â† æ–°ç‰ˆæœ¬

å»ºè®®: åˆ é™¤ core/evaluator/ï¼Œç»Ÿä¸€ä½¿ç”¨ evaluation/
```

**é—®é¢˜2: æ¢¯åº¦è®¡ç®—é€»è¾‘é‡å¤**

```
å½“å‰:
  llama3_unbalanced_pruning_gqa_aware.py: ç¬¬450-520è¡Œ
  llama3_global_pruning.py: ç¬¬410-500è¡Œ

éƒ½åœ¨åš:
  - åŠ è½½æ ·æœ¬
  - å‰å‘ä¼ æ’­
  - åå‘ä¼ æ’­
  - æ¢¯åº¦ç´¯åŠ 

å»ºè®®: æå–åˆ° core/utils/gradient_utils.py
```

```python
# ç»Ÿä¸€æ¥å£
def compute_gradients(model, tokenizer, num_samples, seq_len, method='taylor'):
    """
    ç»Ÿä¸€çš„æ¢¯åº¦è®¡ç®—æ¥å£

    Returns:
        gradients: Dict[param_name, gradient]
        hessian_diag: Dict[param_name, hessian] (ä»…taylor_2nd)
    """
```

**é—®é¢˜3: æ¨¡å‹åŠ è½½é€»è¾‘é‡å¤**

```
å½“å‰: æ¯ä¸ªè„šæœ¬éƒ½æœ‰è‡ªå·±çš„åŠ è½½é€»è¾‘

å»ºè®®: core/utils/model_utils.py
```

```python
def load_model_and_tokenizer(model_path, device='auto', torch_dtype=torch.float16):
    """ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½"""
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch_dtype,
        device_map=device,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    return model, tokenizer
```

#### 3.4.2 æ¥å£è®¾è®¡

**é—®é¢˜: å‚æ•°ä¼ é€’æ··ä¹±**

```python
# å½“å‰: 50+ ä¸ªå‘½ä»¤è¡Œå‚æ•°
parser.add_argument('--base_model', ...)
parser.add_argument('--pruning_ratio', ...)
parser.add_argument('--pruning_distribution', ...)
parser.add_argument('--layer_importance_method', ...)
# ... è¿˜æœ‰46ä¸ªå‚æ•°

å»ºè®®: é…ç½®æ–‡ä»¶ + å‘½ä»¤è¡Œç»“åˆ
```

```yaml
# configs/llama3_prune_25pct.yaml
model:
  base_model: /newdata/LLMs/Llama-3-8B-Instruct
  save_name: llama3_pruned_25pct

pruning:
  method: unbalanced  # or 'global'
  ratio: 0.25
  distribution: "2:8"

  layer_importance:
    method: removal
    samples: 50

  strategy:
    type: inverse
    weight: 1.0
    freeze_top_n: 3

finetuning:
  enabled: true
  method: lora
  lora_r: 8
  lora_alpha: 16
  lr: 2e-4
  epochs: 3
  samples: 1000

evaluation:
  test_before: true
  test_after: true
  seq_len: 512
```

```python
# ä½¿ç”¨
python llama3_prune.py --config configs/llama3_prune_25pct.yaml \
    --pruning.ratio 0.30  # å‘½ä»¤è¡Œè¦†ç›–
```

#### 3.4.3 æ–‡æ¡£ç»´æŠ¤

**é—®é¢˜: æ–‡æ¡£åˆ†æ•£ä¸”éƒ¨åˆ†è¿‡æ—¶**

```
å½“å‰:
  README.md              - ç”¨æˆ·å¿«é€Ÿå…¥é—¨
  CLAUDE.md              - AIåŠ©æ‰‹æŒ‡å— (éå¸¸è¯¦ç»†ï¼Œ700+è¡Œ)
  GLOBAL_PRUNING_GUIDE.md - å…¨å±€å‰ªæ
  PARAMETERS_GUIDE.md     - å‚æ•°è¯´æ˜
  SEARCH_EXAMPLE.md       - æœç´¢ç¤ºä¾‹
  DATASET_SELECTION_GUIDE.md - æ•°æ®é›†
  IMPLEMENTATION_SUMMARY.md - å®ç°æ€»ç»“
  core/README.md          - æ¨¡å—æ–‡æ¡£
  evaluation/README.md    - è¯„ä¼°æ–‡æ¡£
  evaluation/QUICKSTART.md - è¯„ä¼°å¿«é€Ÿå…¥é—¨

é—®é¢˜:
  - ä¿¡æ¯é‡å¤
  - æ›´æ–°æ—¶å®¹æ˜“é—æ¼
  - æ–°æ‰‹ä¸çŸ¥é“ä»å“ªçœ‹èµ·

å»ºè®®: é‡æ„ä¸ºåˆ†å±‚æ–‡æ¡£ç»“æ„
```

```
docs/
â”œâ”€â”€ README.md                 # é¡¹ç›®æ€»è§ˆ + å¿«é€Ÿé“¾æ¥
â”œâ”€â”€ quickstart.md             # 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
â”œâ”€â”€ user-guide/              # ç”¨æˆ·æŒ‡å—
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ basic-usage.md
â”‚   â”œâ”€â”€ parameter-tuning.md
â”‚   â””â”€â”€ troubleshooting.md
â”œâ”€â”€ developer-guide/         # å¼€å‘è€…æŒ‡å—
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api-reference.md
â”‚   â””â”€â”€ contributing.md
â”œâ”€â”€ tutorials/               # æ•™ç¨‹
â”‚   â”œâ”€â”€ layer-pruning.md
â”‚   â”œâ”€â”€ global-pruning.md
â”‚   â””â”€â”€ hyperparameter-search.md
â””â”€â”€ reference/               # å‚è€ƒ
    â”œâ”€â”€ cli-reference.md
    â””â”€â”€ config-schema.md
```

#### 3.4.4 æµ‹è¯•è¦†ç›–

**é—®é¢˜: ç¼ºä¹è‡ªåŠ¨åŒ–æµ‹è¯•**

```
å½“å‰: æ— tests/ç›®å½•ï¼Œæ‰€æœ‰æµ‹è¯•éƒ½æ˜¯æ‰‹åŠ¨çš„

å»ºè®®: æ·»åŠ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
```

```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_gqa_aware.py
â”‚   â”‚   â””â”€â”€ test_compute_gqa_group_importance()
â”‚   â”œâ”€â”€ test_layer_analyzer.py
â”‚   â”‚   â””â”€â”€ test_compute_layer_pruning_rates()
â”‚   â””â”€â”€ test_finetuner.py
â”‚       â””â”€â”€ test_lora_setup()
â”‚
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_layer_pruning_pipeline.py
â”‚   â””â”€â”€ test_global_pruning_pipeline.py
â”‚
â””â”€â”€ fixtures/
    â””â”€â”€ tiny_model/  # ç”¨äºæµ‹è¯•çš„å°æ¨¡å‹ï¼ˆå¦‚GPT-2 smallï¼‰
```

```python
# tests/unit/test_gqa_aware.py
import pytest
import torch
from core.methods.gqa_aware import compute_gqa_group_importance

def test_compute_gqa_group_importance():
    # åˆ›å»ºå‡çš„layer
    class FakeLayer:
        def __init__(self):
            self.self_attn = FakeAttention()

    class FakeAttention:
        def __init__(self):
            self.q_proj = FakeLinear(4096, 4096)  # 32 heads Ã— 128 dim
            self.k_proj = FakeLinear(4096, 1024)  # 8 heads Ã— 128 dim
            # ...

    layer = FakeLayer()

    # è®¾ç½®æ¢¯åº¦
    # ...

    # æµ‹è¯•
    importance = compute_gqa_group_importance(layer, head_dim=128, gqa_ratio=4)

    assert importance.shape == (8,)  # 8 KV heads
    assert (importance >= 0).all()   # é‡è¦æ€§éè´Ÿ
```

---

## 4. è¯„ä¼°æŒ‡æ ‡ä¸Baseline

### 4.1 æ€§èƒ½æŒ‡æ ‡

#### 4.1.1 Perplexity (PPL)

**å®šä¹‰**:
```
PPL = exp(-(1/N) Î£ log P(x_i | x_<i))

ç›´è§‰: æ¨¡å‹å¯¹æµ‹è¯•é›†çš„"å›°æƒ‘ç¨‹åº¦"
  - PPLè¶Šä½ï¼Œæ¨¡å‹è¶Šç¡®ä¿¡ï¼ˆæ€§èƒ½è¶Šå¥½ï¼‰
  - PPL=1: å®Œç¾é¢„æµ‹
  - PPL=âˆ: å®Œå…¨éšæœº
```

**è¯„ä¼°æ•°æ®é›†**:
- **WikiText-2**: å­¦æœ¯æ ‡å‡†ï¼Œçº¦2M tokens
- **C4**: æ›´å¤§è§„æ¨¡ï¼Œæ›´çœŸå®

**å®ç°**:
```python
# evaluation/metrics/ppl.py
class PPLMetric:
    def __init__(self, model, tokenizer, datasets=['wikitext2'], seq_len=128):
        """
        è®¡ç®—PPL

        æ–¹æ³•: æ»‘åŠ¨çª—å£
        - å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºé•¿åº¦ä¸ºseq_lençš„å—
        - å¯¹æ¯å—è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶
        - å¹³å‡åå–exp
        """
        self.model = model
        self.tokenizer = tokenizer
        self.seq_len = seq_len

        # è®¡ç®—
        nlls = []
        for batch in dataloader:
            with torch.no_grad():
                outputs = model(batch, labels=batch)
                nll = outputs.loss * batch.size(1)  # æ¢å¤æ€»NLL
                nlls.append(nll)

        ppl = torch.exp(torch.stack(nlls).sum() / total_tokens)
```

**å…¸å‹å€¼**ï¼ˆLLaMA-3-8B, WikiText-2ï¼‰:

| æ¨¡å‹çŠ¶æ€ | PPL | è¯´æ˜ |
|---------|-----|------|
| åŸå§‹æ¨¡å‹ | 12-15 | Baseline |
| å‰ªæ15% (2:8) | 14-17 | è½»åº¦é€€åŒ– |
| å‰ªæ25% (2:8) | 45-85 | æ˜æ˜¾é€€åŒ–ï¼Œéœ€å¾®è°ƒ |
| å‰ªæ25% + LoRA | 15-25 | æ¢å¤å¤§éƒ¨åˆ†æ€§èƒ½ |
| å‰ªæ40% (2:8) | 150+ | ä¸¥é‡é€€åŒ– |

#### 4.1.2 Zero-Shotå‡†ç¡®ç‡

**ä»»åŠ¡**: æ— éœ€å¾®è°ƒç›´æ¥æ¨ç†

```python
# evaluation/metrics/performance.py
def evaluate_zero_shot(model, tokenizer, tasks=['arc_easy', 'hellaswag']):
    """
    Zero-shotè¯„ä¼°

    å¸¸ç”¨ä»»åŠ¡:
    - ARC (AI2 Reasoning Challenge)
    - HellaSwag (å¸¸è¯†æ¨ç†)
    - PIQA (ç‰©ç†å¸¸è¯†)
    - WinoGrande (ä»£è¯æ¶ˆæ­§)
    """
    # ä½¿ç”¨lm-evaluation-harnessåº“
    results = evaluator.simple_evaluate(
        model=model,
        tasks=tasks,
        num_fewshot=0
    )
```

**å…¸å‹ç»“æœ** (LLaMA-3-8B):

| ä»»åŠ¡ | åŸå§‹ | å‰ªæ25% | å‰ªæ25%+å¾®è°ƒ |
|------|------|---------|--------------|
| ARC-easy | 78.2% | 72.5% | 75.8% |
| HellaSwag | 60.1% | 54.3% | 58.2% |

#### 4.1.3 æ¨¡å‹å¤§å°ä¸æ•ˆç‡

```python
# evaluation/metrics/efficiency.py
def measure_efficiency(model):
    """
    æ•ˆç‡æŒ‡æ ‡:
    1. å‚æ•°é‡
    2. æ¨ç†ååé‡ (tokens/sec)
    3. å†…å­˜å ç”¨ (GB)
    4. å»¶è¿Ÿ (ms/token)
    """
    # å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())

    # ååé‡
    start_time = time.time()
    outputs = model.generate(inputs, max_new_tokens=100)
    throughput = 100 / (time.time() - start_time)

    # å†…å­˜
    memory_mb = torch.cuda.max_memory_allocated() / 1024**2
```

**å…¸å‹å€¼** (LLaMA-3-8B, single A100):

| æŒ‡æ ‡ | åŸå§‹ | å‰ªæ25% | æ”¹è¿› |
|------|------|---------|------|
| å‚æ•°é‡ | 8.03B | 6.02B | -25% |
| æ¨¡å‹æ–‡ä»¶ | 16GB | 12GB | -25% |
| æ¨ç†æ˜¾å­˜ | 18GB | 14GB | -22% |
| ååé‡ | 42 tok/s | 51 tok/s | +21% |

### 4.2 å®éªŒBaselineè®¾ç½®

#### 4.2.1 æ ‡å‡†å¯¹æ¯”ç»„

```
Baseline 1: åŸå§‹æ¨¡å‹
  - LLaMA-3-8B-Instruct
  - ä¸åšä»»ä½•å‰ªæ
  - ä½œä¸ºæ€§èƒ½ä¸Šç•Œ

Baseline 2: å‡åŒ€å‰ªæ
  - æ‰€æœ‰å±‚å‰ªæç‡ç›¸åŒ
  - Attention:MLP = 5:5
  - éªŒè¯éå‡è¡¡ç­–ç•¥çš„å¿…è¦æ€§

Baseline 3: æ–‡çŒ®æ–¹æ³•
  - LLM-Pruner (NIPS'23)
  - Wanda (ICLR'24)
  - éªŒè¯æœ¬æ–¹æ³•çš„ä¼˜åŠ¿
```

#### 4.2.2 æ¶ˆèå®éªŒ

**å®éªŒ1: å±‚é‡è¦æ€§è¯„ä¼°æ–¹æ³•**
```
å˜é‡: --layer_importance_method
  - removal (é€å±‚ç§»é™¤)
  - activation (æ¿€æ´»ç»Ÿè®¡)

å›ºå®š: pruning_ratio=0.25, distribution=2:8, strategy=inverse
```

**å®éªŒ2: å‰ªæç­–ç•¥**
```
å˜é‡: --pruning_strategy
  - inverse (é‡è¦å±‚å°‘å‰ª)
  - proportional (é‡è¦å±‚å¤šå‰ª)
  - uniform (å‡åŒ€)

å›ºå®š: pruning_ratio=0.25, distribution=2:8
```

**å®éªŒ3: Attention:MLPåˆ†å¸ƒ**
```
å˜é‡: --pruning_distribution
  - 0:10, 2:8, 5:5, 8:2, 10:0

å›ºå®š: pruning_ratio=0.25, strategy=inverse
```

**å®éªŒ4: å±‚å†»ç»“**
```
å˜é‡: --freeze_top_n_layers
  - 0, 1, 3, 5, 8

å›ºå®š: pruning_ratio=0.25, distribution=2:8
```

**å®éªŒ5: é‡è¦æ€§è®¡ç®—æ–¹æ³•ï¼ˆå…¨å±€å‰ªæï¼‰**
```
å˜é‡: --importance_method
  - taylor (ä¸€é˜¶)
  - taylor_2nd (äºŒé˜¶)
  - wanda

å›ºå®š: pruning_ratio=0.25
```

#### 4.2.3 å…¸å‹å®éªŒç»“æœï¼ˆç¤ºä¾‹ï¼‰

**æ•°æ®**: LLaMA-3-8B, WikiText-2, å‰ªæç‡25%

| é…ç½® | PPL | é€€åŒ– | å‚æ•°é‡ | è¯´æ˜ |
|------|-----|------|--------|------|
| **åŸå§‹æ¨¡å‹** | 12.3 | - | 8.03B | Baseline |
| **å‡åŒ€å‰ªæ (5:5)** | 142.4 | +1057% | 6.02B | æœ€å·® |
| **éå‡è¡¡ (2:8, uniform)** | 98.7 | +702% | 6.02B | æ”¹è¿› |
| **éå‡è¡¡ (2:8, inverse)** | 83.8 | +581% | 6.02B | **æ›´å¥½** |
| **éå‡è¡¡ (2:8, inverse, freeze=3)** | 73.6 | +498% | 6.02B | **æœ€ä¼˜** |
| **å…¨å±€å‰ªæ (taylor)** | 65.2 | +430% | 6.02B | ç†è®ºæœ€ä¼˜ |
| **å…¨å±€å‰ªæ (taylor_2nd)** | 58.9 | +379% | 6.02B | **æœ€å…ˆè¿›** |
| **+ LoRAå¾®è°ƒ (r=16)** | 18.5 | +50% | 6.02B | æ¥è¿‘åŸå§‹ |

**å…³é”®å‘ç°**:
1. éå‡è¡¡ > å‡åŒ€ (83.8 vs 142.4)
2. å±‚å†»ç»“æœ‰æ•ˆ (73.6 vs 83.8)
3. å…¨å±€å‰ªææœ€ä¼˜ (58.9 vs 73.6)
4. äºŒé˜¶Taylor > ä¸€é˜¶ (58.9 vs 65.2)
5. LoRAå¾®è°ƒå¤§å¹…æ¢å¤ (18.5 vs 58.9)

---

## 5. é‡æ„æ–¹å‘å»ºè®®

### 5.1 çŸ­æœŸä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

#### 5.1.1 ä»£ç æ¸…ç†

**ä¼˜å…ˆçº§: é«˜**

**ä»»åŠ¡æ¸…å•**:
```
â–¡ åˆ é™¤ core/evaluator/ (å·²è¿ç§»åˆ°evaluation/)
â–¡ ç»Ÿä¸€æ¢¯åº¦è®¡ç®—é€»è¾‘ â†’ core/utils/gradient_utils.py
â–¡ ç»Ÿä¸€æ¨¡å‹åŠ è½½é€»è¾‘ â†’ core/utils/model_utils.py
â–¡ æ•´åˆé‡å¤çš„å‚æ•°è§£æä»£ç 
â–¡ æ¸…ç†æœªä½¿ç”¨çš„å¯¼å…¥å’Œå‡½æ•°
```

**é¢„æœŸæ”¶ç›Š**:
- ä»£ç é‡å‡å°‘ ~15%
- ç»´æŠ¤æˆæœ¬é™ä½

#### 5.1.2 é…ç½®æ–‡ä»¶æ”¯æŒ

**ä¼˜å…ˆçº§: é«˜**

**å®ç°**:
```python
# core/utils/config.py
import yaml
from dataclasses import dataclass, field
from typing import Optional

@dataclass
class PruningConfig:
    method: str = 'unbalanced'  # or 'global'
    ratio: float = 0.25
    distribution: str = '2:8'

    layer_importance_method: str = 'removal'
    layer_importance_samples: int = 50

    strategy: str = 'inverse'
    strategy_weight: float = 1.0
    freeze_top_n: int = 0

@dataclass
class FineTuningConfig:
    enabled: bool = False
    method: str = 'lora'
    lora_r: int = 8
    lora_alpha: int = 16
    lr: float = 2e-4
    epochs: int = 3

@dataclass
class ExperimentConfig:
    model: ModelConfig
    pruning: PruningConfig
    finetuning: FineTuningConfig
    evaluation: EvaluationConfig

    @classmethod
    def from_yaml(cls, path):
        with open(path) as f:
            data = yaml.safe_load(f)
        return cls(**data)

    @classmethod
    def from_args(cls, args):
        # ä»argparseè½¬æ¢
        ...

# ä½¿ç”¨
config = ExperimentConfig.from_yaml('configs/my_exp.yaml')
# å‘½ä»¤è¡Œè¦†ç›–
config.pruning.ratio = args.pruning_ratio or config.pruning.ratio
```

**é¢„æœŸæ”¶ç›Š**:
- å®éªŒå¯å¤ç°æ€§æå‡
- å‚æ•°ç®¡ç†æ›´æ¸…æ™°
- æ”¯æŒå®éªŒæ¨¡æ¿

#### 5.1.3 æ—¥å¿—å¢å¼º

**ä¼˜å…ˆçº§: ä¸­**

**æ”¹è¿›ç‚¹**:
```python
# core/utils/logger.py (å¢å¼ºç‰ˆ)
import wandb  # å¯é€‰

class EnhancedLogger(LoggerWithDepth):
    def __init__(self, ..., use_wandb=False, wandb_project=None):
        super().__init__(...)

        if use_wandb:
            wandb.init(project=wandb_project, config=config)

    def log_metric(self, name, value, step=None):
        """è®°å½•æŒ‡æ ‡"""
        self.log(f"{name}: {value}")

        if self.use_wandb:
            wandb.log({name: value}, step=step)

    def log_model_stats(self, model, stage=''):
        """è®°å½•æ¨¡å‹ç»Ÿè®¡"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters()
                              if p.requires_grad)

        self.log(f"[{stage}] Total params: {total_params:,}")
        self.log(f"[{stage}] Trainable: {trainable_params:,}")

        if self.use_wandb:
            wandb.log({
                f'{stage}/total_params': total_params,
                f'{stage}/trainable_params': trainable_params
            })

# ä½¿ç”¨
logger = EnhancedLogger(..., use_wandb=True, wandb_project='llama3-pruning')
logger.log_model_stats(model, stage='before_pruning')
logger.log_metric('ppl', 12.3, step=0)
```

**é¢„æœŸæ”¶ç›Š**:
- å®éªŒè¿½è¸ªå¯è§†åŒ–
- ä¸å›¢é˜Ÿå…±äº«ç»“æœæ›´æ–¹ä¾¿

### 5.2 ä¸­æœŸé‡æ„ï¼ˆ1-2æœˆï¼‰

#### 5.2.1 ç»Ÿä¸€å‰ªææ¥å£

**ä¼˜å…ˆçº§: é«˜**

**åŠ¨æœº**: å½“å‰å±‚çº§å‰ªæå’Œå…¨å±€å‰ªææ˜¯ä¸¤ä¸ªç‹¬ç«‹è„šæœ¬ï¼Œéš¾ä»¥å¯¹æ¯”å’Œåˆ‡æ¢

**è®¾è®¡**:
```python
# core/pruning/pruner.py
from abc import ABC, abstractmethod

class BasePruner(ABC):
    """å‰ªæå™¨åŸºç±»"""

    def __init__(self, model, tokenizer, config, logger=None):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.logger = logger

    @abstractmethod
    def analyze(self):
        """åˆ†æé˜¶æ®µ: è®¡ç®—é‡è¦æ€§"""
        pass

    @abstractmethod
    def prune(self):
        """å‰ªæé˜¶æ®µ: æ‰§è¡Œå‰ªæ"""
        pass

    def run(self):
        """å®Œæ•´æµç¨‹"""
        self.logger.log("å¼€å§‹åˆ†æ...")
        self.analyze()

        self.logger.log("å¼€å§‹å‰ªæ...")
        self.prune()

        return self.model

class LayerwisePruner(BasePruner):
    """å±‚çº§å‰ªæå™¨"""

    def analyze(self):
        # å±‚é‡è¦æ€§åˆ†æ
        analyzer = LayerImportanceAnalyzer(...)
        self.layer_importance = analyzer.measure_layer_importance(...)

        # è®¡ç®—å‰ªæç‡
        calculator = UnbalancedStructuredPruningCalculator(...)
        self.pruning_rates = calculator.compute_layer_pruning_rates(...)

    def prune(self):
        for layer_idx in range(num_layers):
            # é€å±‚å‰ªæ
            ...

class GlobalPruner(BasePruner):
    """å…¨å±€å‰ªæå™¨"""

    def analyze(self):
        # è®¡ç®—æ¢¯åº¦
        self.gradients, self.hessian = compute_gradients(...)

        # æ„å»ºå…¨å±€è¡¨
        self.group_table = build_global_group_table(...)
        self.groups_to_prune = select_groups_to_prune(...)

    def prune(self):
        apply_global_pruning(self.model, self.groups_to_prune)

# å·¥å‚æ¨¡å¼
def create_pruner(method, model, config, logger):
    if method == 'layerwise':
        return LayerwisePruner(model, config, logger)
    elif method == 'global':
        return GlobalPruner(model, config, logger)
    else:
        raise ValueError(f"Unknown method: {method}")

# ä½¿ç”¨
pruner = create_pruner(config.pruning.method, model, config, logger)
pruned_model = pruner.run()
```

**é¢„æœŸæ”¶ç›Š**:
- æ–¹æ³•åˆ‡æ¢æ›´å®¹æ˜“ï¼ˆæ”¹ä¸€ä¸ªé…ç½®å­—æ®µï¼‰
- ä»£ç å¤ç”¨æå‡
- æ˜“äºæ·»åŠ æ–°æ–¹æ³•

#### 5.2.2 PipelineæŠ½è±¡

**ä¼˜å…ˆçº§: ä¸­**

**è®¾è®¡**:
```python
# core/pipeline.py
class PruningPipeline:
    """å®Œæ•´çš„å‰ªææµç¨‹"""

    def __init__(self, config):
        self.config = config
        self.logger = create_logger(config)

    def run(self):
        # 1. åŠ è½½æ¨¡å‹
        self.logger.log("åŠ è½½æ¨¡å‹...")
        model, tokenizer = load_model_and_tokenizer(self.config.model.path)

        # 2. è¯„ä¼°baseline (å¯é€‰)
        if self.config.evaluation.test_before:
            self.logger.log("è¯„ä¼°baseline...")
            baseline_ppl = evaluate_ppl(model, tokenizer)
            self.logger.log_metric('baseline_ppl', baseline_ppl)

        # 3. å‰ªæ
        self.logger.log("å‰ªæ...")
        pruner = create_pruner(self.config.pruning.method, model, self.config, self.logger)
        model = pruner.run()

        # 4. è¯„ä¼°å‰ªæå (å¯é€‰)
        if self.config.evaluation.test_after_prune:
            self.logger.log("è¯„ä¼°å‰ªæå...")
            pruned_ppl = evaluate_ppl(model, tokenizer)
            self.logger.log_metric('pruned_ppl', pruned_ppl)

        # 5. å¾®è°ƒ (å¯é€‰)
        if self.config.finetuning.enabled:
            self.logger.log("å¾®è°ƒ...")
            finetuner = FineTuner(model, tokenizer, self.config.finetuning, self.logger)
            model = finetuner.run()

            # è¯„ä¼°å¾®è°ƒå
            finetuned_ppl = evaluate_ppl(model, tokenizer)
            self.logger.log_metric('finetuned_ppl', finetuned_ppl)

        # 6. ä¿å­˜
        if self.config.save_model:
            save_model(model, self.logger.env_dir)

        return model

# ä½¿ç”¨
config = ExperimentConfig.from_yaml('configs/my_exp.yaml')
pipeline = PruningPipeline(config)
model = pipeline.run()
```

**é¢„æœŸæ”¶ç›Š**:
- ä¸»è„šæœ¬æåº¦ç®€åŒ– (åªéœ€å‡ è¡Œä»£ç )
- æµç¨‹æ ‡å‡†åŒ–
- æ˜“äºæ‰©å±•æ–°æ­¥éª¤

#### 5.2.3 æ¨¡å—åŒ–é‡è¦æ€§è®¡ç®—

**ä¼˜å…ˆçº§: ä¸­**

**åŠ¨æœº**: å½“å‰Taylor/Wanda/Taylor_2ndçš„ä»£ç åˆ†æ•£

**è®¾è®¡**:
```python
# core/importance/calculators.py
class ImportanceCalculator(ABC):
    @abstractmethod
    def compute(self, module, inputs) -> torch.Tensor:
        """
        è®¡ç®—é‡è¦æ€§

        Args:
            module: è¦è¯„ä¼°çš„æ¨¡å— (nn.Linear)
            inputs: è¾“å…¥æ•°æ®ï¼ˆç”¨äºæ¿€æ´»æˆ–æ¢¯åº¦ï¼‰

        Returns:
            importance: Tensorï¼Œæ¯ä¸ªç¥ç»å…ƒ/é€šé“çš„é‡è¦æ€§
        """
        pass

class TaylorFirstOrderCalculator(ImportanceCalculator):
    def compute(self, module, inputs):
        # ç¡®ä¿æœ‰æ¢¯åº¦
        assert module.weight.grad is not None

        importance = (module.weight * module.weight.grad).abs()
        return importance.sum(dim=1)  # æŒ‰è¡Œæ±‚å’Œ (è¾“å‡ºç»´åº¦)

class TaylorSecondOrderCalculator(ImportanceCalculator):
    def __init__(self):
        self.hessian_diag = {}

    def accumulate_hessian(self, model):
        """ç´¯åŠ Hessianå¯¹è§’çº¿"""
        for name, param in model.named_parameters():
            if param.grad is not None:
                if name not in self.hessian_diag:
                    self.hessian_diag[name] = torch.zeros_like(param, device='cpu')
                self.hessian_diag[name] += (param.grad ** 2).cpu()

    def compute(self, module, inputs):
        first_order = (module.weight * module.weight.grad).abs()

        # è·å–Hessian
        hess = self.hessian_diag.get(module_name, 0).to(module.weight.device)
        second_order = 0.5 * (module.weight ** 2 * hess).abs()

        importance = first_order + second_order
        return importance.sum(dim=1)

class WandaCalculator(ImportanceCalculator):
    def __init__(self):
        self.activations = {}

    def register_hooks(self, model):
        """æ³¨å†Œæ¿€æ´»æ”¶é›†hooks"""
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                module.register_forward_hook(self._make_hook(name))

    def _make_hook(self, name):
        def hook(module, input, output):
            act = input[0].detach().abs().mean(dim=(0, 1))
            self.activations[name] = act.cpu()
        return hook

    def compute(self, module, inputs):
        activation = self.activations[module_name].to(module.weight.device)
        importance = (module.weight.abs() * activation).sum(dim=1)
        return importance

# å·¥å‚
def create_importance_calculator(method):
    if method == 'taylor':
        return TaylorFirstOrderCalculator()
    elif method == 'taylor_2nd':
        return TaylorSecondOrderCalculator()
    elif method == 'wanda':
        return WandaCalculator()

# ä½¿ç”¨
calculator = create_importance_calculator('taylor_2nd')

# å‡†å¤‡æ•°æ®
calculator.accumulate_hessian(model)  # å¤šä¸ªbatch

# è®¡ç®—
for layer in model.layers:
    importance = calculator.compute(layer.mlp.gate_proj, None)
    # å‰ªæ...
```

**é¢„æœŸæ”¶ç›Š**:
- æ˜“äºæ·»åŠ æ–°çš„é‡è¦æ€§æ–¹æ³• (å¦‚Fisherä¿¡æ¯)
- ä»£ç å¤ç”¨
- æ¥å£æ¸…æ™°

### 5.3 é•¿æœŸè§„åˆ’ï¼ˆ3-6æœˆï¼‰

#### 5.3.1 æ”¯æŒæ›´å¤šæ¨¡å‹

**å½“å‰**: ä»…æ”¯æŒLLaMA-3

**ç›®æ ‡**: æ³›åŒ–åˆ°å…¶ä»–æ¶æ„

```python
# core/models/model_adapter.py
class ModelAdapter(ABC):
    """æ¨¡å‹é€‚é…å™¨"""

    @abstractmethod
    def get_num_layers(self, model):
        pass

    @abstractmethod
    def get_layer(self, model, idx):
        pass

    @abstractmethod
    def get_attention_module(self, layer):
        pass

    @abstractmethod
    def get_mlp_module(self, layer):
        pass

    @abstractmethod
    def get_gqa_config(self, model):
        """è¿”å› (num_q_heads, num_kv_heads, head_dim)"""
        pass

class LlamaAdapter(ModelAdapter):
    def get_num_layers(self, model):
        return len(model.model.layers)

    def get_layer(self, model, idx):
        return model.model.layers[idx]

    def get_attention_module(self, layer):
        return layer.self_attn

    def get_mlp_module(self, layer):
        return layer.mlp

    def get_gqa_config(self, model):
        layer = self.get_layer(model, 0)
        attn = self.get_attention_module(layer)
        return (attn.num_heads,
                attn.num_key_value_heads,
                attn.head_dim)

class MistralAdapter(ModelAdapter):
    # ç±»ä¼¼å®ç°
    ...

# è‡ªåŠ¨æ£€æµ‹
def create_adapter(model):
    if isinstance(model, LlamaForCausalLM):
        return LlamaAdapter()
    elif isinstance(model, MistralForCausalLM):
        return MistralAdapter()
    # ...

# ä½¿ç”¨
adapter = create_adapter(model)
num_layers = adapter.get_num_layers(model)
for i in range(num_layers):
    layer = adapter.get_layer(model, i)
    attn = adapter.get_attention_module(layer)
    # ç»Ÿä¸€å¤„ç†...
```

**æ”¯æŒæ¨¡å‹**:
- âœ… LLaMA-3 (å·²æ”¯æŒ)
- ğŸ¯ Mistral
- ğŸ¯ Qwen
- ğŸ¯ Phi
- ğŸ¯ Gemma

#### 5.3.2 åˆ†å¸ƒå¼å‰ªæ

**åŠ¨æœº**: å¤§æ¨¡å‹ï¼ˆ70B+ï¼‰å•å¡æ”¾ä¸ä¸‹

**æ–¹æ¡ˆ**:
```python
# core/distributed/ddp_pruner.py
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DistributedPruner(BasePruner):
    def __init__(self, model, config, rank, world_size):
        super().__init__(model, config)
        self.rank = rank
        self.world_size = world_size

        # æ¨¡å‹å¹¶è¡Œ
        self.model = DDP(model, device_ids=[rank])

    def analyze(self):
        if self.rank == 0:
            # ä¸»è¿›ç¨‹: æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„é‡è¦æ€§
            all_importance = [None] * self.world_size
            dist.gather_object(local_importance, all_importance, dst=0)

            # å…¨å±€æ±‡æ€»
            global_importance = aggregate(all_importance)

            # å¹¿æ’­å†³ç­–
            dist.broadcast_object_list([groups_to_prune], src=0)
        else:
            # å·¥ä½œè¿›ç¨‹: å‘é€æœ¬åœ°é‡è¦æ€§
            dist.gather_object(local_importance, dst=0)

            # æ¥æ”¶å‰ªæå†³ç­–
            groups_to_prune = [None]
            dist.broadcast_object_list(groups_to_prune, src=0)

    def prune(self):
        # æ¯ä¸ªè¿›ç¨‹æ‰§è¡Œç›¸åŒçš„å‰ªææ“ä½œ
        apply_pruning(self.model.module, groups_to_prune)

        # åŒæ­¥
        dist.barrier()

# å¯åŠ¨
# torchrun --nproc_per_node=4 main.py --distributed
```

#### 5.3.3 è‡ªåŠ¨åŒ–å®éªŒç®¡ç†

**ç›®æ ‡**: ç±»ä¼¼AutoMLçš„è‡ªåŠ¨å®éªŒ

```python
# core/auto/search.py
from optuna import create_study

def objective(trial):
    """Optunaä¼˜åŒ–ç›®æ ‡"""
    # é‡‡æ ·è¶…å‚æ•°
    pruning_ratio = trial.suggest_float('pruning_ratio', 0.15, 0.40)
    attn_ratio = trial.suggest_float('attn_ratio', 0.0, 5.0)
    mlp_ratio = 10.0 - attn_ratio
    freeze_n = trial.suggest_int('freeze_top_n', 0, 8)
    lora_r = trial.suggest_int('lora_r', 4, 32)

    # æ„å»ºé…ç½®
    config = ExperimentConfig(
        pruning=PruningConfig(ratio=pruning_ratio,
                             distribution=f'{attn_ratio:.1f}:{mlp_ratio:.1f}',
                             freeze_top_n=freeze_n),
        finetuning=FineTuningConfig(enabled=True, lora_r=lora_r)
    )

    # è¿è¡Œå®éªŒ
    pipeline = PruningPipeline(config)
    model = pipeline.run()

    # è¯„ä¼°
    ppl = evaluate_ppl(model, tokenizer)

    return ppl  # æœ€å°åŒ–PPL

# æœç´¢
study = create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print(f"æœ€ä¼˜é…ç½®: {study.best_params}")
print(f"æœ€ä¼˜PPL: {study.best_value}")
```

**é¢„æœŸæ”¶ç›Š**:
- è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜è¶…å‚
- å‡å°‘äººå·¥è°ƒå‚æ—¶é—´
- æ¢ç´¢æ›´å¤§çš„å‚æ•°ç©ºé—´

#### 5.3.4 å¯è§£é‡Šæ€§åˆ†æ

**åŠ¨æœº**: ç†è§£å‰ªæä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Œå“ªäº›ç¥ç»å…ƒè¢«å‰ªæ‰äº†

**åŠŸèƒ½**:
```python
# core/analysis/explainability.py
class PruningAnalyzer:
    """å‰ªæå¯è§£é‡Šæ€§åˆ†æ"""

    def analyze_pruned_components(self, pruning_record):
        """åˆ†æè¢«å‰ªæ‰çš„ç»„ä»¶"""
        # 1. å±‚åˆ†å¸ƒ
        layer_dist = defaultdict(int)
        for group in pruning_record:
            layer_dist[group['layer_idx']] += 1

        plot_layer_distribution(layer_dist)

        # 2. Attention vs MLP
        attn_count = sum(1 for g in pruning_record if g['type'] == 'attention')
        mlp_count = sum(1 for g in pruning_record if g['type'] == 'mlp')

        print(f"å‰ªæ‰çš„Attentionç»„: {attn_count}")
        print(f"å‰ªæ‰çš„MLPé€šé“: {mlp_count}")

        # 3. é‡è¦æ€§åˆ†å¸ƒ
        importance_values = [g['importance'] for g in pruning_record]
        plot_importance_histogram(importance_values)

    def visualize_attention_patterns(self, model, layer_idx, text):
        """å¯è§†åŒ–å‰ªæåçš„æ³¨æ„åŠ›æ¨¡å¼"""
        # è·å–æ³¨æ„åŠ›æƒé‡
        with torch.no_grad():
            outputs = model(text, output_attentions=True)
            attn = outputs.attentions[layer_idx]

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        plot_attention_heatmap(attn)

    def compare_activations(self, original_model, pruned_model, text):
        """å¯¹æ¯”å‰ªæå‰åçš„æ¿€æ´»"""
        # æ”¶é›†æ¿€æ´»
        orig_acts = collect_activations(original_model, text)
        pruned_acts = collect_activations(pruned_model, text)

        # å¯¹æ¯”æ¯å±‚
        for layer_idx in range(num_layers):
            similarity = cosine_similarity(orig_acts[layer_idx],
                                          pruned_acts[layer_idx])
            print(f"Layer {layer_idx} æ¿€æ´»ç›¸ä¼¼åº¦: {similarity:.3f}")

# ä½¿ç”¨
analyzer = PruningAnalyzer()
analyzer.analyze_pruned_components(pruning_record)
analyzer.visualize_attention_patterns(pruned_model, layer_idx=15, text="Hello world")
analyzer.compare_activations(original_model, pruned_model, text="...")
```

---

## 6. æ€»ç»“ä¸å»ºè®®

### 6.1 é¡¹ç›®æ ¸å¿ƒä»·å€¼

æœ¬é¡¹ç›®åœ¨LLMç»“æ„åŒ–å‰ªæé¢†åŸŸçš„**ç‹¬ç‰¹è´¡çŒ®**:

1. **GQAæ¶æ„æ„ŸçŸ¥**: é¦–ä¸ªç³»ç»Ÿæ€§å¤„ç†Grouped Query Attentionå‰ªæçš„æ–¹æ¡ˆ
2. **å±‚çº§éå‡è¡¡ç­–ç•¥**: åŸºäºå±‚é‡è¦æ€§çš„æ™ºèƒ½å‰ªæç‡åˆ†é…
3. **å…¨å±€æ€§ä»·æ¯”ä¼˜åŒ–**: å°†å‰ªæå»ºæ¨¡ä¸ºåˆ†æ•°èƒŒåŒ…é—®é¢˜ï¼Œç†è®ºæœ€ä¼˜
4. **è‡ªåŠ¨åŒ–è¶…å‚æœç´¢**: æ™ºèƒ½åŒå‘æœç´¢ + æ—©åœï¼Œæ•ˆç‡æå‡30%
5. **å®Œæ•´å·¥å…·é“¾**: ä»å‰ªæåˆ°å¾®è°ƒåˆ°è¯„ä¼°çš„ç«¯åˆ°ç«¯æ–¹æ¡ˆ

### 6.2 å½“å‰çŠ¶æ€è¯„ä¼°

**ä¼˜åŠ¿**:
- âœ… æ ¸å¿ƒç®—æ³•å…ˆè¿›ï¼ˆå…¨å±€å‰ªæ + äºŒé˜¶Taylorï¼‰
- âœ… æ–‡æ¡£è¯¦ç»†å®Œæ•´ï¼ˆCLAUDE.md 700+è¡Œï¼‰
- âœ… å®éªŒå¯å¤ç°
- âœ… æ¨¡å—åŒ–è®¾è®¡è‰¯å¥½

**åŠ£åŠ¿**:
- âŒ ä»£ç æœ‰å†—ä½™ï¼ˆæ¢¯åº¦è®¡ç®—ã€æ¨¡å‹åŠ è½½ç­‰é‡å¤ï¼‰
- âŒ ç¼ºä¹è‡ªåŠ¨åŒ–æµ‹è¯•
- âŒ ä»…æ”¯æŒLLaMAæ¶æ„
- âŒ é…ç½®ç®¡ç†ä¸å¤Ÿçµæ´»ï¼ˆå‘½ä»¤è¡Œå‚æ•°è¿‡å¤šï¼‰

### 6.3 é‡æ„ä¼˜å…ˆçº§

**P0 (å¿…åš)**: ç«‹å³æ”¹è¿›ç”¨æˆ·ä½“éªŒ
- é…ç½®æ–‡ä»¶æ”¯æŒ (YAML)
- ä»£ç å»é‡ï¼ˆæ¢¯åº¦ã€åŠ è½½ç­‰ï¼‰
- ç»Ÿä¸€å‰ªææ¥å£ï¼ˆBasePrunerï¼‰

**P1 (é‡è¦)**: æå‡å·¥ç¨‹è´¨é‡
- å•å…ƒæµ‹è¯•è¦†ç›–
- PipelineæŠ½è±¡
- æ—¥å¿—å¢å¼ºï¼ˆWandbé›†æˆï¼‰

**P2 (é•¿æœŸ)**: æ‰©å±•èƒ½åŠ›
- å¤šæ¨¡å‹æ”¯æŒ
- åˆ†å¸ƒå¼å‰ªæ
- è‡ªåŠ¨åŒ–è¶…å‚æœç´¢ï¼ˆOptunaï¼‰
- å¯è§£é‡Šæ€§åˆ†æ

### 6.4 å­¦æœ¯ä¸å·¥ç¨‹æ–¹å‘

**å­¦æœ¯æ–¹å‘**:
1. **ç†è®º**: ä¸ºä»€ä¹ˆå…¨å±€å‰ªæä¼˜äºå±‚çº§å‰ªæï¼Ÿèƒ½å¦è¯æ˜åˆ†æ•°èƒŒåŒ…çš„è¿‘ä¼¼æ¯”ï¼Ÿ
2. **æ–¹æ³•**: æ¢ç´¢ä¸‰é˜¶Taylorã€Fisherä¿¡æ¯ç­‰æ›´å…ˆè¿›çš„é‡è¦æ€§åº¦é‡
3. **æ¶ˆè**: ç³»ç»Ÿæ€§è¯„ä¼°æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼ˆå±‚é‡è¦æ€§ã€å†»ç»“ã€åˆ†å¸ƒç­‰ï¼‰
4. **æ³›åŒ–**: æ‰©å±•åˆ°å…¶ä»–æ¶æ„ï¼ˆEncoder-Decoderã€MOEç­‰ï¼‰

**å·¥ç¨‹æ–¹å‘**:
1. **æ€§èƒ½**: GPU kernelä¼˜åŒ–ï¼ŒåŠ é€Ÿå‰ªæå’Œå¾®è°ƒ
2. **å¯ç”¨æ€§**: Web UIç•Œé¢ï¼Œå¯è§†åŒ–å®éªŒå¯¹æ¯”
3. **éƒ¨ç½²**: å‰ªææ¨¡å‹çš„é‡åŒ–ã€è’¸é¦ã€éƒ¨ç½²ä¼˜åŒ–
4. **è‡ªåŠ¨åŒ–**: AutoMLå¼çš„è¶…å‚æœç´¢å’Œæ¨¡å‹é€‰æ‹©

### 6.5 è¡ŒåŠ¨è®¡åˆ’ï¼ˆå»ºè®®ï¼‰

**Week 1-2: ä»£ç æ¸…ç†**
```
Day 1-3:  åˆ é™¤å†—ä½™ï¼Œç»Ÿä¸€æ¥å£
Day 4-5:  é…ç½®æ–‡ä»¶æ”¯æŒ
Day 6-7:  PipelineæŠ½è±¡
Day 8-10: å•å…ƒæµ‹è¯•ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰
```

**Week 3-4: å®éªŒéªŒè¯**
```
Day 1-5:  æ¶ˆèå®éªŒï¼ˆå±‚é‡è¦æ€§ã€å†»ç»“ã€åˆ†å¸ƒç­‰ï¼‰
Day 6-10: å…¨å±€å‰ªæ vs å±‚çº§å‰ªæ å…¨é¢å¯¹æ¯”
```

**Month 2: åŠŸèƒ½æ‰©å±•**
```
Week 1: å¤šæ¨¡å‹æ”¯æŒï¼ˆMistral, Qwenï¼‰
Week 2: Wandbé›†æˆï¼Œå®éªŒè¿½è¸ª
Week 3: å¯è§£é‡Šæ€§åˆ†æå·¥å…·
Week 4: æ–‡æ¡£é‡æ„ï¼Œå‘å¸ƒåšå®¢
```

**Month 3+: é«˜çº§ç‰¹æ€§**
```
- åˆ†å¸ƒå¼å‰ªæ
- è‡ªåŠ¨åŒ–è¶…å‚æœç´¢
- Web UI
- è®ºæ–‡æ’°å†™
```

---

## é™„å½•

### A. å…³é”®å…¬å¼æ€»ç»“

**ä¸€é˜¶Tayloré‡è¦æ€§**:
$$I_u = \sum_{\theta \in u} \left| \theta \cdot \frac{\partial \mathcal{L}}{\partial \theta} \right|$$

**äºŒé˜¶Tayloré‡è¦æ€§**:
$$I_u = \sum_{\theta \in u} \left| \theta \cdot \frac{\partial \mathcal{L}}{\partial \theta} \right| + \frac{1}{2} \left| \theta^2 \cdot H_{diag} \right|$$

**Hessianå¯¹è§’çº¿è¿‘ä¼¼**:
$$H_{diag} \approx \frac{1}{N} \sum_{i=1}^N g_i^2$$

**Wandaé‡è¦æ€§**:
$$I_u = \sum_{\theta \in u} \left| \theta \cdot A_u \right|$$

**æ€§ä»·æ¯”å¾—åˆ†**:
$$S_u = \frac{I_u}{C_u}$$

**å±‚çº§å‰ªæç‡ï¼ˆinverseç­–ç•¥ï¼‰**:
$$r_i = \frac{w_i}{\sum_j w_j} \cdot r_{target} \cdot L, \quad w_i = \frac{1}{(I_i + \epsilon)^\alpha}$$

### B. ä»£ç ç‰‡æ®µé€ŸæŸ¥

**åŠ è½½æ¨¡å‹**:
```python
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float16, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(path)
```

**è®¡ç®—PPL**:
```python
from evaluation.metrics.ppl import PPLMetric
ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], seq_len=128, device='cuda')
print(ppl)  # {'wikitext2 (wikitext-2-raw-v1)': 12.34}
```

**GQAç»„å‰ªæ**:
```python
from core.methods.gqa_aware import compute_gqa_group_importance, prune_attention_by_gqa_groups
importance = compute_gqa_group_importance(layer, head_dim=128, gqa_ratio=4)
keep_indices = importance.argsort(descending=True)[:target_kv_heads]
prune_attention_by_gqa_groups(layer, keep_indices, head_dim=128, gqa_ratio=4)
```

**LoRAå¾®è°ƒ**:
```python
from core.trainer.finetuner import FineTuner
finetuner = FineTuner(model, tokenizer, device='cuda', logger=logger)
finetuner.finetune(method='lora', lora_r=8, lora_alpha=16, lr=2e-4, epochs=3)
```

### C. å‚è€ƒèµ„æº

**ç›¸å…³è®ºæ–‡**:
- LLM-Pruner (NIPS'23): ç»“æ„åŒ–å‰ªæ + LoRAæ¢å¤
- Wanda (ICLR'24): æ— éœ€å¾®è°ƒçš„å‰ªæ
- SparseGPT (ICML'23): OBS-basedå‰ªæ
- ShortGPT (arXiv'23): æ·±åº¦å‰ªæ

**å·¥å…·åº“**:
- torch-pruning: é€šç”¨å‰ªææ¡†æ¶
- lm-evaluation-harness: LLMè¯„ä¼°
- PEFT: LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒ
- Optuna: è¶…å‚æ•°ä¼˜åŒ–

---

**æ–‡æ¡£ç»“æŸ**

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æIssueæˆ–PRï¼
