# Evaluation æ¨¡å—ä½¿ç”¨æŒ‡å—

GAQ-Aware-Prune çš„ç»Ÿä¸€è¯„ä¼°æ¨¡å—ï¼Œæ”¯æŒæ€§èƒ½å’Œæ•ˆç‡æŒ‡æ ‡çš„å…¨é¢è¯„ä¼°ã€‚

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
evaluation/
â”œâ”€â”€ metrics/              # æŒ‡æ ‡æ¨¡å—
â”‚   â”œâ”€â”€ performance.py    # æ€§èƒ½æŒ‡æ ‡ï¼ˆPPL, Zero-shotï¼‰
â”‚   â””â”€â”€ efficiency.py     # æ•ˆç‡æŒ‡æ ‡ï¼ˆé€Ÿåº¦ã€å†…å­˜ï¼‰
â”‚
â”œâ”€â”€ utils/               # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ model_loader.py  # æ¨¡å‹åŠ è½½
â”‚   â””â”€â”€ result_parser.py # ç»“æœè§£æ
â”‚
â”œâ”€â”€ run_evaluation.py    # â­ ç»Ÿä¸€å…¥å£è„šæœ¬
â””â”€â”€ README.md           # æœ¬æ–‡æ¡£
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€è¯„ä¼°ï¼ˆPPLã€é€Ÿåº¦ã€å†…å­˜ï¼‰
pip install torch transformers datasets

# Zero-shot/Few-shotè¯„ä¼°ï¼ˆå¯é€‰ï¼‰
pip install lm-eval
```

---

### 2. è¯„ä¼°å•ä¸ªæ¨¡å‹

```bash
# è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡
python evaluation/run_evaluation.py \
    --model_path prune_log/ours_optimal/pytorch_model.bin \
    --metrics all \
    --output results/ours.json

# åªè¯„ä¼°PPLå’Œé€Ÿåº¦
python evaluation/run_evaluation.py \
    --model_path /newdata/LLMs/Llama-3-8B-Instruct \
    --metrics ppl,speed \
    --output results/original.json

# è‡ªå®šä¹‰è¯„ä¼°é…ç½®
python evaluation/run_evaluation.py \
    --model_path prune_log/ours_optimal/pytorch_model.bin \
    --metrics ppl,zeroshot,speed,memory \
    --ppl_datasets wikitext2,ptb,c4 \
    --zeroshot_tasks hellaswag,piqa,winogrande \
    --speed_samples 100 \
    --output results/ours_full.json
```

---

### 3. å¯¹æ¯”å¤šä¸ªæ¨¡å‹

```bash
# é¦–å…ˆè¯„ä¼°å„ä¸ªæ¨¡å‹ï¼ˆç”Ÿæˆ.jsonæ–‡ä»¶ï¼‰
python evaluation/run_evaluation.py --model_path ... --output results/model1.json
python evaluation/run_evaluation.py --model_path ... --output results/model2.json

# ç„¶åç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
python evaluation/run_evaluation.py \
    --compare \
    --model_paths results/model1.json,results/model2.json,results/model3.json \
    --output results/comparison_table.md
```

**è¾“å‡ºç¤ºä¾‹** (`comparison_table.md`):
```markdown
| Metric | Original | Ours | Baseline_Uniform |
|---|---|---|---|
| Parameters (B) | 8.03 | 6.02 | 6.02 |
| PPL (WikiText-2) | 12.34 | 38.46 | 85.30 |
| Avg Zero-shot Acc (%) | 78.50 | 75.20 | 65.10 |
| Throughput (tokens/s) | 125.3 | 168.7 | 169.2 |
| GPU Memory (MB) | 16384 | 12288 | 12288 |
```

---

## ğŸ“Š æ”¯æŒçš„è¯„ä¼°æŒ‡æ ‡

### æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å‚æ•° |
|------|------|------|
| `ppl` | å¤šæ•°æ®é›†PPL | `--ppl_datasets wikitext2,ptb,c4` |
| `zeroshot` | Zero-shotå‡†ç¡®ç‡ | `--zeroshot_tasks hellaswag,piqa,...` |
| `fewshot` | Few-shotå‡†ç¡®ç‡ï¼ˆå¯é€‰ï¼‰| é»˜è®¤MMLU 5-shot |

**æ”¯æŒçš„PPLæ•°æ®é›†**: `wikitext2`, `ptb`, `c4`

**æ”¯æŒçš„Zero-shotä»»åŠ¡**:
- `hellaswag` - å¸¸è¯†æ¨ç†
- `piqa` - ç‰©ç†å¸¸è¯†
- `winogrande` - ä»£è¯æ¶ˆæ­§
- `arc_easy` / `arc_challenge` - ç§‘å­¦é—®ç­”
- `boolq` - æ˜¯éé—®ç­”

---

### æ•ˆç‡æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | è‡ªåŠ¨æµ‹é‡ |
|------|------|---------|
| `speed` | æ¨ç†é€Ÿåº¦ï¼ˆååé‡ã€å»¶è¿Ÿï¼‰| batch_size=1,4,8 |
| `memory` | æ˜¾å­˜å ç”¨ | æ¨¡å‹åŠ è½½+æ¨ç†å³°å€¼ |
| `efficiency` | å…¨é¢æ•ˆç‡è¯„ä¼° | åŒ…å«speed+memory |

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### å•ç‹¬ä½¿ç”¨å„æ¨¡å—

#### 1. æ€§èƒ½æŒ‡æ ‡

```python
from evaluation.metrics.performance import evaluate_ppl, evaluate_zeroshot
from evaluation.utils.model_loader import load_model_and_tokenizer

# åŠ è½½æ¨¡å‹
model, tokenizer = load_model_and_tokenizer('/path/to/model')

# è¯„ä¼°PPL
ppl_results = evaluate_ppl(model, tokenizer, datasets=['wikitext2', 'ptb'])
print(ppl_results)  # {'wikitext2 (wikitext-2-raw-v1)': 38.46, ...}

# è¯„ä¼°Zero-shotï¼ˆéœ€è¦HFæ ¼å¼æ¨¡å‹ï¼‰
zeroshot_results = evaluate_zeroshot('/path/to/model', tasks=['hellaswag', 'piqa'])
print(zeroshot_results)
```

---

#### 2. æ•ˆç‡æŒ‡æ ‡

```python
from evaluation.metrics.efficiency import evaluate_efficiency
from evaluation.utils.model_loader import load_model_and_tokenizer

model, tokenizer = load_model_and_tokenizer('/path/to/model')

# å…¨é¢æ•ˆç‡è¯„ä¼°
efficiency_results = evaluate_efficiency(
    model, tokenizer,
    num_samples=100,
    batch_sizes=[1, 4, 8]
)

print(f"å‚æ•°é‡: {efficiency_results['model_info']['total_params_B']:.2f}B")
print(f"ååé‡: {efficiency_results['speed']['batch_size_1']['throughput_tokens_per_sec']:.1f} tokens/s")
print(f"æ˜¾å­˜: {efficiency_results['memory']['model_memory_mb']:.1f} MB")
```

---

#### 3. æ¨¡å‹åŠ è½½å·¥å…·

```python
from evaluation.utils.model_loader import load_model_and_tokenizer, get_model_info

# åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«HFç›®å½•æˆ–checkpointï¼‰
model, tokenizer = load_model_and_tokenizer(
    'prune_log/xxx/pytorch_model.bin',
    device='cuda',
    torch_dtype=torch.float16
)

# è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
info = get_model_info(model)
print(f"æ€»å‚æ•°: {info['total_params_B']:.2f}B")
print(f"Attentionå‚æ•°: {info['attention_params_M']:.1f}M ({info['attention_ratio']*100:.1f}%)")
print(f"MLPå‚æ•°: {info['mlp_params_M']:.1f}M ({info['mlp_ratio']*100:.1f}%)")
```

---

## ğŸ“ è¾“å‡ºæ ¼å¼

### JSONç»“æœæ–‡ä»¶

```json
{
  "model_path": "prune_log/ours/pytorch_model.bin",
  "timestamp": "2025-11-18T12:00:00",
  "metrics": {
    "model_info": {
      "total_params": 6024195936,
      "total_params_B": 6.02,
      "attention_params_M": 1024.5,
      "mlp_params_M": 4999.7
    },
    "ppl": {
      "wikitext2 (wikitext-2-raw-v1)": 38.46,
      "ptb": 42.31
    },
    "zeroshot": {
      "hellaswag": {"accuracy": 0.752},
      "piqa": {"accuracy": 0.768}
    },
    "avg_zeroshot_acc": 0.760,
    "efficiency": {
      "speed": {
        "batch_size_1": {
          "throughput_tokens_per_sec": 168.7,
          "latency_ms_per_token": 5.93
        }
      },
      "memory": {
        "model_memory_mb": 12288.5,
        "inference_peak_mb": 14563.2
      }
    }
  }
}
```

---

## ğŸ¯ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: è®ºæ–‡å®éªŒ - å®Œæ•´è¯„ä¼°

```bash
# è¯„ä¼°æ‰€æœ‰æ¨¡å‹çš„æ‰€æœ‰æŒ‡æ ‡
for model in original ours baseline1 baseline2; do
    python evaluation/run_evaluation.py \
        --model_path models/${model}/pytorch_model.bin \
        --metrics ppl,zeroshot,speed,memory \
        --output results/${model}.json
done

# ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
python evaluation/run_evaluation.py \
    --compare \
    --model_paths results/original.json,results/ours.json,results/baseline1.json,results/baseline2.json \
    --output paper_table.md
```

---

### åœºæ™¯2: å¿«é€ŸéªŒè¯ - åªæµ‹PPL

```bash
python evaluation/run_evaluation.py \
    --model_path prune_log/test/pytorch_model.bin \
    --metrics ppl \
    --ppl_datasets wikitext2 \
    --output quick_test.json
```

---

### åœºæ™¯3: æ€§èƒ½æ·±å…¥åˆ†æ - å¤šæ•°æ®é›†PPL

```bash
python evaluation/run_evaluation.py \
    --model_path prune_log/ours/pytorch_model.bin \
    --metrics ppl \
    --ppl_datasets wikitext2,ptb,c4 \
    --output ppl_analysis.json
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. Zero-shotè¯„ä¼°é™åˆ¶

**é—®é¢˜**: Zero-shotéœ€è¦HFæ ¼å¼æ¨¡å‹ï¼Œä¸æ”¯æŒç›´æ¥åŠ è½½`.bin` checkpoint

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: å‰ªææ—¶ä¿å­˜ä¸ºå®Œæ•´HFæ ¼å¼
python llama3_unbalanced_pruning_gqa_aware.py \
    --save_model \
    --save_ckpt_log_name my_model  # ä¿å­˜åˆ°prune_log/my_model/

# æ–¹æ³•2: æ‰‹åŠ¨è½¬æ¢checkpoint
# ï¼ˆéœ€è¦é¢å¤–è„šæœ¬ï¼Œæš‚æœªæä¾›ï¼‰
```

---

### 2. æ˜¾å­˜ä¸è¶³

**é—®é¢˜**: è¯„ä¼°æ—¶OOM

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘é€Ÿåº¦æµ‹è¯•æ ·æœ¬æ•°
--speed_samples 20

# æˆ–åªæµ‹è¯•å°batch size
# (ä¿®æ”¹ä»£ç ä¸­çš„batch_sizeså‚æ•°)
```

---

### 3. lm-evalå®‰è£…é—®é¢˜

**é—®é¢˜**: `ModuleNotFoundError: No module named 'lm_eval'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install lm-eval

# å¦‚æœéœ€è¦æœ€æ–°ç‰ˆ
pip install git+https://github.com/EleutherAI/lm-evaluation-harness
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [lm-evaluation-harnessæ–‡æ¡£](https://github.com/EleutherAI/lm-evaluation-harness)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- é¡¹ç›®ä¸»README: `../README.md`

---

## ğŸ¤ è´¡çŒ®

å¦‚éœ€æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡æˆ–æ”¹è¿›ç°æœ‰åŠŸèƒ½ï¼Œè¯·ï¼š
1. åœ¨å¯¹åº”çš„`metrics/`æ¨¡å—ä¸­æ·»åŠ åŠŸèƒ½
2. æ›´æ–°`run_evaluation.py`ä»¥æ”¯æŒæ–°æŒ‡æ ‡
3. æ›´æ–°æœ¬README
